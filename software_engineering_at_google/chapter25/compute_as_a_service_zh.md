# 第25章 计算即服务

作者：Onufry Wojtaszczyk

编辑：Lisa Carey

我并不试图了解计算机。我试着去理解程序。
Barbara Liskov

在完成了编写代码的艰苦工作后，你需要一些硬件来运行它。因此，你要去购买或租用这些硬件。这在本质上就是计算即服务（CaaS），其中 “计算” 是实际运行你的程序所需的计算能力的简称。

本章是关于这个简单的概念--只要给我硬件来运行我的东西[1]--如何映射成一个随着你的组织的发展和成长而生存和扩展的系统。这一章有点长，因为这个话题很复杂，分为四个部分：

* “驯服计算环境” 涵盖了谷歌是如何得出这个问题的解决方案的，并解释了CaaS的一些关键概念。

* “为托管计算编写软件” 展示了托管计算解决方案是如何影响工程师编写软件的。我们认为，“家畜，而不是宠物” /灵活的调度模式是谷歌在过去15年成功的根本，也是软件工程师工具箱中的一个重要工具。

* “CaaS随时间和规模的变化” 深入探讨了谷歌学到的一些经验，即随着组织的成长和演变，关于计算架构的各种选择是如何发挥的。

* 最后，“选择计算服务”主要致力于那些将决定在其组织中使用何种计算服务的工程师。

## 驯服计算环境

谷歌的内部Borg系统[2]是今天许多CaaS架构（如Kubernetes或Mesos）的先驱。为了更好地理解这种服务的特定方面如何满足一个不断增长和发展的组织的需要，我们将追溯Borg的演变过程以及谷歌工程师为驯服计算环境所做的努力。

### 劳作的自动化

想象一下，在世纪之交的时候，你是一个大学的学生。如果你想部署一些新的、漂亮的代码，你会把代码SFTP到大学计算机实验室的一台机器上，SSH进入机器，编译并运行代码。这是一个简单而诱人的解决方案，但随着时间的推移和规模的扩大，它遇到了相当多的问题。然而，由于这大致是许多项目开始时的情况，多个组织最终采用的流程在某种程度上是这个系统的精简演变，至少对于某些任务来说是这样的--机器的数量增加了（所以你SFTP和SSH进入其中许多机器），但基础技术仍然存在。例如，在2002年，谷歌最资深的工程师之一杰夫-迪安（Jeff Dean）就运行一个自动数据处理任务作为发布过程的一部分写了以下内容。

  [运行该任务]是一个后勤、耗时的恶梦。目前，它需要获得一个50多台机器的列表，在这50多台机器中的每一台上启动一个进程，并在这50多台机器中的每一台上监控其进度。如果其中一台机器死亡，没有支持自动将计算迁移到另一台机器上，监测工作的进展是以临时的方式进行的[......]此外，由于进程可以相互干扰，有一个复杂的、由人类实施的“注册”文件来节制机器的使用，这导致了不太理想的调度，以及对稀缺的机器资源的更多争夺

这是谷歌努力驯服计算环境的早期导火索，这很好地解释了简单的解决方案如何在更大的规模下变得不可维护。

### 简单的自动化

有一些简单的事情可以由一个组织可以做从而减轻一些痛苦。将二进制文件部署到50多台机器中的每一台，并在那里启动，这个过程很容易通过一个shell脚本来实现自动化，然后--如果这是一个可重复使用的解决方案--通过一个更强大的代码，用一种更容易维护的语言来执行并行部署（特别是由于 “50多台” 可能会随着时间而增加）。

更有趣的是，对每台机器的监控也可以自动化。最初，负责这个过程的人希望知道（并能够进行干预），如果其中一个副本出了问题。这意味着从进程中输出一些监控指标（如 “进程是活的” 和 “处理的文件数” ）从拥有监控指标的那个进程写到一个共享存储中，或调用一个监控服务，在那里他们可以一目了然地看到反常情况。目前该领域的开源解决方案是，例如，在Graphana或Prometheus等监控工具中设置一个仪表盘。

如果检测到异常，通常的缓解策略是通过SSH进入机器，杀死进程（如果它还活着），然后重新启动它。这很繁琐，可能很容易出错（要确保你连接到正确的机器，并确保杀死正确的进程），而且可以自动化：

* 与其手动监测故障，不如在机器上使用一个代理，检测异常情况（比如 “该进程在过去5分钟内没有报告它是活的” 或 “该进程在过去10分钟内没有处理任何文件” ），并在检测到异常情况时杀死该进程。

* 与其在死亡后登录到机器上再次启动进程，不如将整个执行过程包裹在一个 `"while true; do run && break; done"`的shell脚本中。

在云计算世界中，相当于设置一个自动修复策略（在虚拟机或容器无法通过健康检查后，杀死并重新创建）。

这些相对简单的改进解决了前面描述的Jeff Dean问题的一部分，但不是全部；人类实施的节制，以及移动到一个新机器，需要更多的解决方案。

### 自动调度

下一步自然是将机器分配自动化。这需要第一个真正的 “服务” ，最终将发展成为 “计算即服务” 。也就是说，要实现自动化调度，我们需要一个中央服务，知道它可用的机器的完整列表，并能按需挑选一些未被占用的机器，自动将你的二进制文件部署到这些机器上。这样就不需要手工维护的 “注册” 文件了，而是把维护机器列表的工作交给计算机。这个系统很容易让人联想到早期的时间共享架构。

这个想法的一个自然延伸是将这种调度与对机器故障的反应结合起来。通过扫描机器日志中健康状况不佳的表现的信号（例如，大量磁盘读取错误），我们可以识别出坏掉的机器，信号（向人类）提示需要修理这些机器，并避免在此期间在这些机器上安排任何工作。将消除劳碌的范围进一步扩大，自动化可以在人类参与之前先尝试一些修复方法，比如重启机器，希望错误的东西消失，或者运行一个自动磁盘扫描。

杰夫说的最后一个抱怨是，如果运行的机器坏了，需要人把计算迁移到另一台机器上。这里的解决方案很简单：因为我们已经有了调度自动化和检测机器故障的能力，我们可以简单地让调度器分配一个新的机器，并在这个新机器上重新开始工作，放弃旧的机器。这样做的信号可能来自于机器自省守护程序或对单个进程的监控。

所有这些改进都系统地应对了组织规模的增长。当机群是一台机器时，SFTP和SSH是完美的解决方案，但在数百或数千台机器的规模下，需要自动化来接管。我们开始引用的这句话来自于2002年 “全球工作队列” 的设计文件，这是一个早期的CaaS内部解决方案，用于谷歌的一些工作负载。

### 容器化和多租户

到目前为止，我们隐含地假设了机器和运行在机器上的程序之间是一对一的映射。就计算资源（RAM、CPU）的消耗而言，这在很多方面都是非常低效的：

* 很可能有许多不同类型的作业（具有不同的资源需求），而不是机器类型（具有不同的资源可用性），所以许多作业需要使用相同的机器类型（需要根据其中最大的作业进行配置）。

* 机器的部署需要很长的时间，而程序的资源需求则随着时间的推移而增长。如果获得新的、更大的机器需要花费你的组织几个月的时间，你还需要使它们足够大，以满足在配置新机器所需时间内资源需求的预期增长，这就导致了浪费，因为新机器没有被充分利用起来。[3]

* 即使新机器到了，你还有旧机器（扔掉它们很可能是浪费），所以你必须管理一个不适应你需求的异质机群。

自然的解决方案是为每个程序指定其资源需求（在CPU、内存、磁盘空间方面），然后要求调度器将程序的副本打包到可用的机器池中。

#### 我的邻居的狗在我的内存中吠叫

如果每个人都能很好地发挥，上述的解决方案就能完美地工作。然而，如果我在配置中指定我的数据处理管道的每个副本将消耗一个CPU和200MB的内存，然后由于一个错误，或自然增长，它开始消耗更多资源，它被安排到的机器将耗尽资源。在CPU的情况下，这将导致相邻的服务作业出现延迟；在RAM的情况下，它将导致内核的out-of-memory kills，或者由于磁盘交换而出现可怕的延迟。[4]

同一台计算机上的两个程序也会在其他方面产生互动不良。许多程序都希望它们的依赖关系能以某种特定的版本安装在一台机器上--而这些可能与其他程序的版本要求相冲突。一个程序可能希望某些系统范围内的资源（想想/tmp）可以供自己单独使用。安全是一个问题--一个程序可能正在处理敏感数据，需要确保同一台机器上的其他程序不能访问它。

因此，多租户计算服务必须提供一定程度的隔离，保证一个进程能够安全地进行而不被机器的其他租户干扰。

隔离的一个经典解决方案是使用虚拟机（VM）。然而，这些虚拟机在资源使用（它们需要资源在里面运行一个完整的操作系统）和启动时间（同样，它们需要启动一个完整的操作系统）方面有很大的开销[5]。这使得它们对于资源占用少、运行时间短批量作业的容器化来说成为了一个不那么完美的解决方案。这导致谷歌在2003年设计Borg的工程师们寻找不同的解决方案，最终产生了容器--一种基于cgroups（谷歌工程师在2007年将其纳入Linux内核）和chroot jails、bind mounts和/或union/overlay文件系统进行文件系统隔离的轻型机制。开源容器的实现包括Docker和LMCTFY。

随着时间的推移和组织的发展，越来越多的潜在隔离故障被发现。举个具体的例子，2011年，从事Borg工作的工程师发现，进程ID空间（默认设置为32,000个PID）的耗尽正在成为一种隔离故障，不得不引入对单个副本可产生的进程/线程总数的限制。我们将在本章后面详细介绍这个例子。

#### 权限化和自动扩展

2006年的Borg根据工程师在配置中提供的参数安排工作，如副本数量和资源需求。

从远处看这个问题，要求人类确定资源需求数字的想法有些缺陷：这些数字不是人类每天都在互动的。因此，随着时间的推移，这些配置参数本身就成为低效率的来源。工程师需要花时间在最初的服务启动时确定这些参数，而随着你的组织积累越来越多的服务，确定这些参数的成本也在增加。此外，随着时间的推移，程序在演进（可能会增长），但配置参数并没有跟上。这最终导致了故障的发生--事实证明，随着时间的推移，新版本的资源需求吞噬了留给意外高峰或故障的松弛，而当这种高峰或故障实际发生时，剩余的松弛被证明是不够的。

自然的解决方案是自动设置这些参数。不幸的是，这被证明是出乎意料的棘手。作为一个例子，谷歌最近才达到一个点，即整个Borg一半以上的资源使用量是由自动化合理化决定的。也就是说，尽管这只是一半的使用量，但它是配置中较大的一部分，这意味着大多数工程师不需要关心他们容器大小的繁琐和容易出错的负担。我们认为这是对 “简单的事情应该是容易的，复杂的事情应该是可能的” 这一理念的成功应用--仅仅因为Borg工作负载的某些部分过于复杂，无法通过调整大小来妥善管理，并不意味着在处理简单情况时没有巨大的价值。

### 总结

随着你的组织的成长和你的产品变得更受欢迎，你将在所有这些轴上成长:

* 需要管理的不同应用程序的数量

* 需要运行的应用程序的副本数量

* 最大的应用程序的规模

为了有效地管理规模，需要自动化，使你能够解决所有这些增长轴。随着时间的推移，你应该期待自动化本身变得更多，既要处理新类型的要求（例如，GPU和TPU的调度是Borg在过去10年里发生的一个主要变化），又要处理规模的增加。那些在较小规模下可以手工操作的行动，将需要自动化，以避免组织在负载下的崩溃。

一个例子--谷歌仍在摸索的过渡阶段--就是对我们的数据中心进行自动化管理。十年前，每个数据中心是一个独立的实体。我们手动管理它们。启用一个数据中心是一个复杂的手工过程，需要专门的技能，需要几周的时间（从所有机器准备好的那一刻开始），而且本身就有风险。然而，谷歌管理的数据中心数量的增长意味着我们转向了一种模式，即开启数据中心是一个不需要人工干预的自动化过程。

## 编写管理计算的软件

从手工管理的机器列表到自动调度和正确调整的世界，使谷歌的机群管理变得更加容易，但这也使我们编写和思考软件的方式发生了深刻的变化。

### 失败的架构

想象一下，一个工程师要处理一批100万份文件并验证其正确性。如果处理一个文件需要一秒钟，那么整个工作将需要一台机器大约12天--这可能太长了。所以，我们把工作分散到200台机器上，这样就把运行时间减少到了更容易管理的100分钟。

正如在 “自动调度” 中所讨论的，在Borg世界中，调度员可以单方面杀死200个workers中的一个，并把它移到不同的机器上。[6] “把它移到不同的机器上” 这部分意味着你的worker的新实例可以自动被印出来，不需要人去SSH进入机器，调整一些环境变量或安装软件包。

从 “工程师必须手动监控100个任务中的每一个，并在出现问题时对其进行处理” 到 “系统会被设计成，如果其中一个任务出现问题，它的工作由其他任务来承担，而自动调度器会将其杀死并在新的机器上重新确定” ，这一转变在许多年后通过 “宠物与家畜” 的比喻来描述。[7]

如果你的服务器是一只宠物，当它坏了的时候，会有一个人来看它（通常是在恐慌中），了解出了什么问题，并希望能把它护理好，恢复健康。它很难被取代。如果你的服务器是家畜，你给它们命名为replica001到replica100，如果有一个出现故障，自动化会把它移走，并在其位置上配置一个新的。“家畜” 的显著特点是，很容易为有关工作的新实例盖章--它不需要手动设置，可以完全自动完成。这使得前面描述的自我修复特性成为可能--在发生故障的情况下，自动化可以接管，用一个新的、健康的工作取代不健康的工作，而不需要人工干预。请注意，虽然原来的比喻说的是服务器（虚拟机），但同样适用于容器：如果你能在没有人工干预的情况下从镜像中印出一个新版本的容器，你的自动化就能在需要时自动修复你的服务。

如果你的服务器是宠物，你的维护负担将随着你的机群规模线性增长，甚至超线性增长，而这是任何组织都不应该轻易接受的负担。另一方面，如果你的服务器是家畜，你的系统将能够在故障后恢复到一个稳定的状态，你将不需要花周末的时间来护理宠物服务器或容器恢复健康。

不过，让你的虚拟机或容器成为家畜，并不足以保证你的系统在面对故障时表现良好。有200台机器，其中一个副本被Borg杀死是很有可能发生的，可能不止一次，而且每次都会使整个持续时间延长50分钟（或者不管损失多少处理时间）。为了优雅地处理这个问题，处理的架构需要改变：我们不是静态地分配工作，而是把整个100万份文件分成，比如说1000块，每块1000份文件。每当一个worker完成了一个特定的块，它就报告结果，并拿起另一个块。这意味着，在worker完成某块工作后，但在报告之前死亡的情况下，我们最多损失一个块。幸运的是，这非常符合当时谷歌的标准数据处理架构：在计算开始时，工作并没有平均分配给一组worker；而是在整个处理过程中动态分配，以考虑到worker的失败。

同样，对于服务于用户流量的系统来说，你理想地希望容器的重新调度不会导致错误被提供给用户。Borg调度器在计划因维护原因而重新调度一个容器时，会向容器发出信号，提前通知它的意图。容器可以通过拒绝新的请求做出反应，同时仍有时间完成它正在进行的请求。这反过来要求负载均衡器系统理解 “我不能接受新请求” 的反应（并将流量重定向到其他副本）。

总结：把你的容器或服务器当成家畜，意味着你的服务可以自动恢复到健康状态，但需要额外的努力，以确保它在经历适度的故障率时能够顺利运行。

### 批量与服务

全局工作队列（我们在本章第一节中描述过）解决了谷歌工程师所说的 “批处理作业” 的问题--这些程序被期望完成一些特定的任务（如数据处理），并运行到完成。批量作业的典型例子是日志分析或机器学习模型学习。批量作业与 “服务作业” 形成鲜明对比--这些程序预计将无限期地运行并为传入的请求提供服务，典型的例子是为预先建立的索引中的实际用户搜索查询提供服务的作业。

这两类作业（通常）有不同的特点，[8] 特别是：

* 批量作业主要关心的是处理的吞吐量。服务工作关心的是服务单个请求的延迟。

* 批量作业是短期存在的（几分钟，或最多几个小时）。服务工作通常是长期存在的（默认情况下，只有在新版本发布时才会重新启动）。

* 因为服务工作是长期存在的，所以服务工作更有可能有较长的启动时间。

到目前为止，我们的大多数示例都是关于批处理作业的。 正如我们已经看到的，要使批处理作业适应故障，我们需要确保将工作分散成小块并动态分配给Worker。 Google的规范框架是MapReduce [9]，随后被Flume [10]取代。

从很多方面来说，服务工作比批处理工作更自然地适合于抗故障能力。 他们的工作自然会分成小块（单个用户请求），然后动态分配给Workers。自从服务互联网流量开始以来，就一直采用通过在服务器群集之间进行负载平衡来处理大量请求的策略。

然而，还有多个服务应用程序并不自然地适合这种模式。典型的例子是你直观地描述为某个特定系统的领导者的任何服务器。这样的服务器通常会维护系统的状态(在内存中或其本地文件系统上)，如果运行它的机器宕机，新创建的实例通常将无法重新创建系统状态。 另一个例子是，当你要处理大量数据（超出一台计算机的容量）时，你决定在100台服务器（分别容纳1％的数据）之间分片数据，并处理该部分的请求的数据。 这类似于将工作静态分配给批处理工作人员。 如果其中一台服务器出现故障，你（暂时）将失去为部分数据提供服务的能力。最后一个例子是，系统中的其他部分是否可以通过主机名感知到服务器。 在这种情况下，无论服务器的结构如何，如果此特定主机失去网络连接，系统的其他部分都将无法与它联系 [11]。

### 管理状态

正如前面描述的，管理状态一个共同主题是，在尝试像cattle [12]一样的工作时，状态是问题的根源。每当您替换其中的一个牛工作时，您都会失去所有过程中的状态（以及以前的所有状态）。（如果作业已移至另一台计算机上）。 这意味着正在处理的状态应该被视为暂时的，而“实际存储”需要出现在其他地方。

解决此问题的最简单方法是将所有存储提取到外部存储系统。这意味着任何超出了单个服务请求范围（在服务作业情况下）或处理一个数据块（在批处理情况下）的范围的东西都需要在机器外持久存储中存储。 如果所有的本地状态都是不可变的，那么应用程序具有抗故障能力应该相对轻松一些。

不幸的是，大多数应用程序并不是那么简单。 可能自然想到的一个问题是：“这些持久的存储解决方案是如何实现的？它们可是cattle？ 答案应该是“是”。 持久状态可以由cattle通过状态复制来管理。 在不同的层面上，RAID阵列是一个类似的概念； 我们将磁盘视为瞬态磁盘（接受其中一个磁盘可能已消失的事实），同时仍保持状态。 在服务器世界中，这可以通过保存单个数据的多个副本并进行同步以确保每个数据被复制足够的次数（通常为3到5次）来实现。 请注意，正确设置此设置非常困难（需要某种共识处理方式来处理写操作），因此Google开发了许多专用存储解决方案 [13]，这些存储解决方案使大多数应用程序都采用了所有状态都是瞬态的模型。

Cattle可以使用的其他类型的本地存储包括本地保存的“可重新创建”数据，以改善服务延迟。 缓存是这里最明显的例子：缓存不过是将临时状态存储在临时位置的临时本地存储而已，但该状态的存储库不会一直消失，因此平均而言可以获得更好的性能特征。 Google生产基础架构的主要工作是调配缓存以满足延迟目标，并调配核心应用程序以实现总负载。 这使我们避免了缓存层丢失时的中断，这是因为未缓存的路径已调配来处理总负载（尽管具有更高的延迟）。 但是，这里有一个明显的权衡：在缓存容量丢失时，要在冗余上花多少钱来减轻中断的风险。

与缓存类似，可以在应用程序预热时将数据从外部存储拉到本地，以改善请求服务的等待时间。

在写入的数据多于读取的情况下，使用本地存储的另一种情况是批量写入。 这是监控数据的常用策略（例如，考虑从车队中收集CPU利用率统计信息以指导自动缩放系统），但是可以在部分数据会消失的可接受位置使用它， 要么是因为我们不需要100％的数据覆盖率（这是监控场景），要么是因为可以重新创建消失的数据（这是批处理作业的情况，该批处理作业以块为单位处理数据，并为每个数据写入一些输出块）。 请注意，在许多情况下，即使特定的计算需要花费很长时间，也可以通过将状态周期性地指向持久性存储的检查点，将其划分为较小的时间窗口。

### 连接到一个服务

如前所述，如果系统中的任何内容都以硬编码形式（或在启动时作为配置参数提供）在其上运行程序的主机名，则你的程序副本就不会存在。 但是，要连接到你的应用程序，另一个应用程序确实需要从某个地方获取你的地址。 在哪里？

答案是要有一个额外的中间层。 也就是说，其他应用程序通过某些标识符引用你的应用程序，该标识符在特定的“后端”实例的重新启动期间是持久的。 该标识符可以由计划程序将应用程序放置在特定计算机上时写入的另一个系统来解析。 现在，为了避免在对应用程序发出请求的关键路径上进行分布式存储查找，客户端可能会在启动时查找可以找到你的应用程序的地址，并建立连接，并在后台对其进行监视 。 这通常称为服务发现，许多计算产品都具有内置或模块化解决方案。 大多数此类解决方案还包括某种形式的负载平衡，从而进一步减少了与特定后端的耦合。

此模型的后果是，在某些情况下，你可能需要重复请求，因为与您通信的服务器可能在管理应答之前就已被关闭[14]。重试请求是网络通信（例如，移动应用程序到一个服务器）的标准做法，但是对于诸如服务器与其数据库进行通信之类的事情可能不太直观。 因此，以一种能够妥善处理此类故障的方式来设计服务器的API至关重要。 对于变异请求，处理重复的请求非常棘手。 你要保证的属性是幂等的某种变体，即两次发出请求的结果与一次发出请求的结果相同。 客户端分配的标识符是帮助实现幂等性的一种有用工具：如果你要创建商品（例如，将比萨饼递送到特定地址的订单），则客户端会为该订单分配一些标识符； 并且如果已经记录了带有该标识符的订单，则服务器会认为它是重复请求并报告成功（它也可能会验证该订单的参数是否匹配）。

另一件令人惊讶的事情是，有时调度程序由于某些网络问题而失去与特定计算机的联系。 然后，它确定所有工作都丢失了，并将其重新安排到其他机器上，然后机器又回来了！ 现在，我们在两台不同的计算机上有两个程序，都认为它们是“ replica072”。 它们消除歧义的方法是检查地址解析系统引用了其中一个（另一个应该终止它自己或被终止）。 但对于幂等也有另外一种情况：执行相同工作并发挥相同作用的两个副本是请求重复的另一个潜在来源。

### 一次性代码

先前的大多数讨论都集中在生产质量的作业上，这些作业要么是为用户流量提供服务，要么是生产生产数据的数据处理管道。 但是，软件工程师的生命还涉及运行一次性分析，探索性原型，自定义数据处理管道等。 这些需要计算资源。

通常，工程师的工作站是满足计算资源需求的令人满意的解决方案。 例如，如果要对服务在过去一天产生的1GB日志进行自动浏览以检查是否在错误行B之前总是出现可疑行A，则他们可以下载日志，编写简短的Python 脚本，并运行一两分钟。

但是，如果他们想通过去年生成的1 TB日志（出于类似目的）自动进行略读，那么大约等待一天的时间才能收到结果，这可能是不可接受的。 允许工程师在几分钟内（使用数百个内核）在分布式环境上运行分析的计算服务意味着现在进行分析与明天进行分析之间的区别。 对于需要迭代的任务（例如，如果我需要在查看结果之后进行查询优化），区别可能在于一天完成一次操作与根本不执行。

这种方法有时会引起的一个问题是，允许工程师仅在分布式环境上运行一次性工作可能会浪费资源。 当然，这是一个折衷，但是应该自觉地做出。 工程师运行的处理成本不会比工程师在编写处理代码上花费的时间昂贵得多。 确切的权衡值取决于组织的计算环境及其向工程师支付的费用，但是，千核每小时花费的费用几乎不可能花费一天的工程工作。 在这方面，计算资源类似于标记，我们在本书的开头讨论了这些标记。 对于公司而言，建立一个流程以获取更多的计算资源的机会很少，但是此流程所浪费的工程机会和时间可能要比节省的成本高得多。

也就是说，计算资源与标记不同，因为很容易意外地占用过多资源。 尽管不太可能有人会拿出一千个标记，但完全有可能有人会不经意间写出一个占用一千台机器的程序 [15]。自然的解决方案是为每个工程师设置资源使用配额。 Google使用的一种替代方法是观察到，因为我们免费有效地运行了低优先级批处理工作负载（请参阅稍后的多租户部分），所以我们可以为工程师提供几乎无限量的低优先级批处理工作配额，这已经足够了，适用于大多数一次性工程任务。

## 随着时间和规模的Caas

我们在上面讨论了CaaS在Google上的发展历程以及实现这一目标所需的基本部分-“仅仅给我资源就能运行我的任务”这一简单的任务是如何转化为像Borg这样的实际架构的。 CaaS体系结构如何影响软件在整个时间和规模上的生命周期的几个方面值得仔细研究。

### 容器作为一个抽象

正如我们之前所描述的，容器主要表示为一种隔离机制，这是一种实现多租户的方法，同时将共享一台机器的不同任务之间的干扰降至最低。 这是最初的动机，至少在Google中是这样。 但是，事实证明，容器在抽象化计算环境方面也起着非常重要的作用。

容器提供了已部署软件和运行它的实际计算机之间的抽象边界。 这意味着随着时间的推移，机器会发生变化，只有容器软件（大概由一个团队来管理）才需要进行调整，而应用软件（随组织的发展由每个团队来管理）可以保持不变。

让我们讨论容器化抽象如何允许组织来管理变更的两个示例。

文件系统抽象提供了一种无需管理自定义机器配置即可合并公司未编写的软件的方法。 这可能是组织在其数据中心中运行的开源软件，也可能是组织希望在其CaaS上使用的收购软件。 如果没有文件系统抽象，则在二进制文件上加载期望不同的文件系统布局（例如，在/bin/foo/bar中使用辅助二进制文件）将需要修改队列中所有机器的基本布局，或者对队列进行分段或修改软件（出于许可考虑，这可能很困难，甚至可能无法实现）。

即使这些解决方案可能是可行的，如果导入一个外部软件是在其生命周期内只发生一次的事情，但是如果导入软件成为一种常见的（甚至是很少见的）实践，那也不是一个可持续的解决方案。

某种文件系统抽象也有助于依赖管理，因为它允许软件预先声明和预打包软件需要运行的依赖关系（例如，特定版本的库）。 根据机器上安装的软件的不同，可能会导致泄漏的抽象，迫使每个人都使用相同版本的预编译库，并且即使不是不可能，也很难升级任何组件。

容器还提供了一种简单的方法来管理计算机上的命名资源。典范的例子是网络端口。 其他命名资源包括专门的目标； 例如GPU和其他加速器。

Google最初并未将网络端口作为容器抽象的一部分，因此二进制文件必须自行搜索未使用的端口。 结果，PickUnusedPortOrDie函数在Google C ++代码库中的使用量超过20,000。Docker是在引入Linux名称空间之后构建的，它使用名称空间为容器提供虚拟专用NIC，这意味着应用程序可以在所需的任何端口上进行侦听。 然后，Docker网络堆栈将机器上的端口映射到容器内端口。 Kubernetes最初是建立在Docker之上的，再往前走了一步，并要求网络实现将容器（以Kubernetes的说法是“ pod”）视为可从主机网络获得的“真实” IP地址。 现在，每个应用程序都可以在他们想要的任何端口上进行监听，而不必担心发生冲突。

当处理未设计在特定计算堆栈上运行的软件时，这些改进特别重要。 尽管许多流行的开源程序都有要使用哪个端口的配置参数，但是在如何配置此端口之间并没有一致性。

#### 容器和隐式依赖

与任何抽象一样，Hyrum的隐式依赖定律也适用于容器抽象。 它可能比平常应用得更多，这是因为用户数量众多（在Google，所有生产软件以及许多其他软件都将在Borg上运行），还因为用户在使用文件系统之类的东西时不会觉得自己在使用API。 （甚至很少考虑此API是否稳定，版本等）

为了说明这一点，让我们回到Borg在2011年经历的进程ID空间耗尽的示例。您可能想知道为什么进程ID可以耗尽。 它们不只是可以从32位或64位空间分配的整数ID吗？ 在Linux中，实际上在[0，...，PID_MAX-1]范围内分配它们，其中PID_MAX默认为32,000。 但是，可以通过简单的配置更改（较大的限制）来提高PID_MAX。 问题解决了？

好吧，不。 根据Hyrum的定律，在Borg上运行的进程的PID限制在0 ... 32,000范围内这一事实成为人们开始依赖的隐式API保证； 例如，日志存储过程取决于PID可以以五位数字存储的事实，并且因为记录名称超过了允许的最大长度而中断了六位数的PID。 解决该问题成为了一个漫长的，分为两个阶段的项目。 首先，临时限制单个容器可以使用的PID数量（这样，单个泄漏线程的工作就不会使整个计算机无法使用）。 其次，为线程和进程分配PID空间。（因为事实证明，很少有用户依赖于分配给线程的PID的32,000保证，而不是依赖于进程。因此，我们可以增加线程的限制并将其保持在32,000的进程。）第三阶段是引入PID Borg的名称空间，为每个容器提供自己的完整PID空间。 可以预见的（再次是希鲁姆定律），假设三元组{hostname，timestamp，pid}唯一地标识一个进程，如果引入了PID名称空间，这将破坏很多系统。 八年后，仍在努力确定所有这些位置并进行修复（并向后移植任何相关数据）。

这里的重点不是您应该在PID名称空间中运行容器。尽管这是个好主意，但这不是有趣的课程。 建造Borg的容器时，PID名称空间不存在。 即使他们做到了，也不能指望2003年设计Borg的工程师意识到引入它们的价值。甚至到现在，一台机器上肯定还有一些资源没有被充分隔离，这有一天可能会引起问题。 这突显了设计一个容器系统的挑战，该容器系统将随着时间的推移而被证明是可维护的，因此，使用由更广泛的社区开发和使用的容器系统的价值也很重要，在这些社区中，其他人已经遇到了这些类型的问题，并且已经吸取了教训。

### 一劳永逸

如前所述，原始的WorkQueue设计仅针对某些批处理作业，最终所有工作共享一个由WorkQueue管理的服务器池，并且为服务提供了不同的体系结构，每个特定的服务任务均在其自己的位置运行， 专用服务器池。 对于每种工作负载，等效的开源程序将运行一个单独的Kubernetes集群（为所有批处理作业添加一个池）。

Borg项目于2003年启动，旨在（最终成功地）构建一种将这些分散的池合并为一个大池的计算服务。 Borg的池涵盖了服务和批处理作业，并成为任何数据中心中的唯一池（等效于为每个地理位置的所有工作负载运行一个大型Kubernetes集群）。 这里有两个重要的效率提升值得讨论。

第一个是服务的机器变成了cattle（Borg设计文档所说的：“机器是匿名的：只要具有正确的特性，程序就不在乎它们在哪台机器上运行”）。 如果每个管理服务工作的团队必须管理自己的计算机池（他们自己的集群），那么维护和管理该池的组织开销将被应用到这些团队中的每个团队。 随着时间的流逝，这些池的管理实践将随着时间的推移而发生变化，从而使公司范围内的更改（例如迁移到新的服务器体系结构或切换数据中心）变得越来越复杂。 统一的管理基础架构（即针对组织中所有工作负载的通用计算服务）使Google避免了这种线性扩展因子。 机队中的物理机没有不同的管理方法，只有Borg [16]。

第二个比较微妙，可能不适用于每个组织，但与Google息息相关。 批处理和服务工作的独特需求被证明是相辅相成的。 服务作业通常需要超额配置，因为它们需要有能力为用户流量提供服务，而不会显着降低延迟，即使在使用高峰或部分基础架构中断的情况下也是如此。 这意味着仅运行正在服务的作业的计算机将无法充分利用。 尝试通过过度使用机器来利用该空闲时间是很诱人的，但这首先就破坏了该空闲时间的目的，因为如果发生尖峰/中断，我们将无法获得所需的资源。

然而，这个推理只适用于提供作业! 如果我们在一台机器上有许多服务作业，这些作业请求的RAM和CPU之和等于机器的总大小，那么就不能再在机器上放置更多的服务作业，即使资源的实际利用率仅为容量的30%。但我们可以(在Borg,会把批处理作业的剩余70%,如果任何的服务工作需要内存或CPU,我们将收回它的批处理作业(通过冻结他们的CPU或杀死的RAM)。因为批处理作业对吞吐量感兴趣(以跨越数百个Worker的总量来衡量，而不是针对单个任务)，而且它们的单个副本无论如何都是cattle，它们将非常乐意吸收服务于这些作业的空闲能力。

根据给定计算机池中工作负载的形状，这意味着要么所有批处理工作负载都有效地在空闲资源上运行（因为无论如何我们都是在服务作业的空闲状态下为它们付费），或者所有正在处理的作业负载都是有效地仅为他们使用的资源付费，而不是为他们的抗故障能力所需的松弛容量付费（因为批处理作业正在该松弛空间中运行）。 在大多数情况下，对于Google而言，事实证明我们可以免费有效地批量运行。

#### 多租户服务作业

早先，我们讨论了计算服务必须满足的一些要求才能适合运行服务作业。 如前所述，由通用计算解决方案管理服务作业有多个优点，但这也带来了挑战。 一个值得重复的特殊要求是发现服务，在第528页上的“连接到服务”中进行了讨论。例如，当我们想将托管计算解决方案的范围扩展到服务任务时，还有许多其他新要求：

- 需要限制对作业的重新安排：虽然可以终止并重新启动批处理作业的50％副本是可行的（因为这只会在处理过程中造成短暂的故障，而我们真正关心的是吞吐量），但这不太可能被接受杀死并重新启动服务作业的50％副本（因为剩余的作业可能太少，无法在等待重新启动的作业再次出现时为用户流量提供服务）。
- 批处理作业通常可以在没有警告的情况下被杀死。 我们丢失的是一些已经执行的处理，可以重做。 当服务作业被毫无预兆地杀死时，我们可能会冒一些面向用户的流量返回错误或（最好）增加延迟的风险。 最好提前几秒钟发出警告，以便作业可以完成正在处理的请求，而不接受新请求。

由于上述效率原因，Borg涵盖了批处理作业和服务作业，但是多种计算产品将这两个概念分开——通常是，用于批处理作业的共享计算机池，以及用于服务于作业的专用，稳定的计算机池。但是，不管两种类型的工作是否使用相同的计算体系结构，这两组作业都可以像对待cattle一样受益。

### 提交的配置

Borg调度程序接收要在单元中运行的复制服务或批处理作业的配置，作为远程过程调用（RPC）的内容。 服务运营商可以使用命令行界面（CLI）来管理该服务，该命令行界面发送这些RPC，并将CLI的参数存储在共享文档中或在其头部中。

通常，依靠文档和部落知识而不是提交给存储库的代码很少是一个好主意，因为文档和部落知识都倾向于随着时间的流逝而恶化（请参阅第3章）。 但是，演进的下一个自然步骤-将CLI的执行包装在本地开发的脚本中-仍然不如使用专用的配置语言来指定服务的配置。

随着时间的流逝，逻辑服务的运行时状态通常会超出一个数据中心中跨多个轴的一组复制容器的范围：

- 将它的存在分布在多个数据中心上（既用于用户亲和力又用于抗故障能力）。
- 除了生产环境/配置之外，它还将具有暂存和开发环境。
- 它将以附加服务的形式累积其他不同类型的其他复制容器，例如服务随附的内存缓存。

如果可以使用标准化配置语言来表达此复杂的设置，从而简化了标准操作（例如“将我的服务更新为二进制的新版本，但占用的容量不超过5％”，则可以简化服务的管理） 在任何给定时间”）。

标准化的配置语言提供了标准的配置，其他团队可以轻松地将其包含在服务定义中。 与往常一样，我们强调这种标准配置在时间和规模上的价值。 如果每个团队都编写不同的自定义代码片段来支持其Memcached服务，则很难执行组织范围内的任务，例如换用新的Memcache实现（例如出于性能或许可原因）或推送安全更新。 所有的Memcache部署。 还要注意，这种标准化的配置语言是部署自动化的必要条件（请参阅第24章）。

## 选择一个计算服务

任何组织都不太可能重蹈谷歌的覆辙，从零开始构建自己的计算体系结构。现在，现代计算产品既可以在开源世界中使用(比如Kubernetes或Mesos，或者在不同的抽象级别上使用OpenWhisk或Knative)，也可以作为公共云管理产品使用(同样，在不同的复杂性级别上，从谷歌云平台的管理实例组或亚马逊Web服务弹性计算云[Amazon EC2]自动伸缩;到类似Borg的托管容器，如微软Azure Kubernetes Service [AKS]或谷歌Kubernetes Engine [GKE];到AWS Lambda或谷歌的Cloud Functions等无服务器产品)。

但是，大多数组织都会选择计算服务，就像Google在内部所做的一样。请注意，计算基础架构具有较高的锁定因子。 原因之一是因为将以一种利用系统所有特性的方式来编写代码（希鲁姆定律）； 因此，例如，如果您选择基于VM的产品，则团队将调整其特定的VM映像； 并且如果您选择特定的基于容器的解决方案，团队将调出集群管理器的API。 如果您的体系结构允许代码将虚拟机（或容器）当作宠物对待，那么团队将这样做，然后很难找到依赖于将它们像牛（甚至不同形式的宠物）一样对待的解决方案。

要显示甚至如何锁定计算解决方案的最小细节，请考虑Borg如何运行用户在配置中提供的命令。 在大多数情况下，该命令将执行二进制文件（可能后跟多个参数）。 但是，为了方便起见，Borg的作者还包括了传递shell脚本的可能性； 例如，while true；do ./my_binary; done [17]，尽管二进制执行可以通过简单的fork-and-exec（Borg所做的）来完成，但是shell脚本需要由像Bash这样的shell运行。 因此，Borg实际上执行了/ usr / bin / bash -c $ USER_COMMAND，它在简单的二进制执行中也起作用。

你可能会认为这不是一个有风险的改变;毕竟，我们控制着环境，我们知道这两个二进制文件都存在，所以这应该不会不起作用。实际上，这并不奏效的原因是Borg工程师并不是第一个注意到运行Bash的额外内存开销的人。一些团队很有创意，他们希望限制内存的使用，并用一段自定义编写的“执行第二个参数”代码替换(在他们的自定义文件系统覆盖层中)Bash命令。当然,这些团队非常清楚自己的内存使用,所以当Borg团队改变了过程运动员使用灰(不覆盖的自定义代码),他们的内存使用增加(因为它开始包括灰使用而不是自定义代码的使用),这引起警报,回滚更改,还有一定程度的不愉快。

计算服务选择难以随时间更改的另一个原因是，任何计算服务选择最终都会被庞大的帮助服务生态系统所包围，这些生态系统包括日志记录，监视，调试，警报，可视化，实时分析， 配置语言和元语言，用户界面等。 这些工具将需要作为计算服务更改的一部分进行重写，即使对于中型或大型组织来说，即使理解和枚举这些工具也可能是一个挑战。

因此，计算体系结构的选择很重要。 与大多数软件工程选择一样，这需要权衡取舍。 让我们讨论一些。

### 集中化与定制

从计算堆栈的管理开销的角度（以及从资源效率的角度），组织可以做的最好的事情就是采用单个CaaS解决方案来管理其整个机器群，并为每个人仅使用可用的工具。 这确保了随着组织的发展，管理团队的成本仍然可控。 这条路径基本上就是Google对Borg所做的。

#### 定制化的需求

然而，一个成长中的组织会有越来越多的不同需求。例如，当谷歌在2012年推出谷歌计算引擎（“虚拟机即服务”的公共云产品）时，就像谷歌的大多数其他产品一样，其虚拟机是由Borg管理的。这意味着每个虚拟机都运行在一个由Borg控制的独立容器中。然而，这种“牛”的任务管理方式并不适合云计算的工作负载，因为每个特定的容器实际上是某个特定用户正在运行的虚拟机，而云计算的用户通常不会把虚拟机当作“牛”。[18]

调和这种差异需要双方做大量的工作。云计算组织确保支持虚拟机的实时迁移；也就是说，能够在一台机器上运行的虚拟机，在另一台机器上启动该虚拟机的副本，使该副本成为一个完美的镜像，并最终能将所有流量重定向到该副本，而不会造成明显的服务不可用期。[19]另一方面，Borg必须进行调整，以避免随意杀死包含虚拟机的容器（以提供时间将虚拟机的内容迁移到新的机器上）。同时，鉴于整个迁移过程更加昂贵，Borg的调度算法为了减少需要重新调度的风险而进行了优化。[20]当然，这些修改只针对运行云工作负载的机器，导致谷歌内部计算产品出现很小，但仍然很明显的分叉。

一个不同的例子来自于搜索，也一样导致了产品分叉。2011年左右，一个为谷歌搜索网络流量服务的副本容器在本地磁盘上建立了一个巨大的索引，存储了谷歌网络索引中不常被访问的部分（更常见的查询由其他容器的内存缓存提供支持）。在一台特定的机器上建立这个索引需要多个硬盘的容量，并且需要几个小时来填入数据。然而，在当时，Borg假设如果某个特定容器的数据所在的任何一块磁盘坏了，该容器将无法继续运行，需要重新安排到不同的机器上。这种组合（与其他硬件相比，机械硬盘的故障率相对较高）造成了严重的可用性问题；容器总是宕机，然后又要花很长时间才能重新启动。为了解决这个问题，Borg必须增加容器自己处理磁盘故障的能力，而不使用Borg的默认处理方式。同时，搜索团队必须调整流程，以便在部分数据丢失的情况下继续运行。

其他方面的分叉，涵盖了文件系统、内存控制、分配和访问、CPU/内存定位、特殊硬件、特殊调度约束等领域，导致Borg的API变得庞大而笨重，各种行为的交叉点变得难以预测，更难测试。没有人真正知道，如果一个容器同时请求进行处理容器置换的特殊云操作和处理磁盘故障的特定搜索操作，是否会发生预期的事情（在许多情况下，甚至不清楚“预期”是什么意思）。

2012年后，Borg团队花了大量时间来清理Borg的API。[21]更令人担忧的是那些被多个容器使用的功能。不清楚是否有意为之，在项目之间复制配置文件的过程导致了原本只针对高级用户的功能发生了扩散。某些功能被引入了白名单，以限制它们的传播，并明确地将它们标记为仅适用于高级用户。然而，清理工作仍在进行，一些变化（如使用标签来识别容器组）仍未完全完成。[22]

就像通常的权衡一样，虽然有一些方法可以投入精力，获得一些定制化的好处，同时避免最坏的问题（比如前面提到的权力功能的白名单），但最终还是要做出艰难的选择。这些选择通常以多个小问题的形式出现：我们应该接受增加显式（或更糟糕的隐式）API以适应我们基础设施的特定用户，还是给该用户带来极大不便，但保持更高的一致性？

### 抽象层次: Serverless

谷歌对计算环境演进的描述很容易被解读为一个增加和改善抽象的故事。更高级版本的Borg承担了更多的管理责任，并将容器与底层环境更多地隔离。这很容易让人觉得这是一个简单的故事：更多的抽象是好的；更少的抽象是坏的。

当然，事情没有那么简单。这里的情况很复杂，有多种产品。在 "驯服计算环境 "中，我们讨论了从处理在裸机上运行的宠物（要么是你的组织拥有的，要么是从主机托管中心租来的）到管理容器的进展情况。在这两者之间，作为一个替代路径，是基于虚拟机的产品，其中虚拟机可以从更灵活地替代裸机（在基础设施即服务产品中，如谷歌计算引擎GCE或亚马逊EC2）发展到更重地替代容器（具有自动缩放、权限调整和其他管理工具）。

根据谷歌的经验，选择管理“牛”（而不是宠物）是规模管理的解决方案。重申一下，如果你的每个团队在每个数据中心只需要一台宠物机，那么你的管理成本将随着你的组织的而超扩展线性上升（因为团队的数量和一个团队所占用的数据中心的数量都可能增长）。而在选择管理“牛”之后，容器是管理的自然选择。它们更轻（意味着更小的资源开销和启动时间），而且可配置，如果你需要为特定类型的工作负载提供专门的硬件，如果你选择的话，你可以轻松地打洞访问。

虚拟机作为“牛”的优势主要在于能够带来我们自己的操作系统，如果你的工作负载需要一套多样化的操作系统来运行，这一点很重要。多个组织也会有管理虚拟机的预存经验，以及基于虚拟机的预存配置和工作负载，因此可能会选择使用虚拟机而不是容器来减轻迁移成本。

#### 什么是 SERVERLESS?

Serverlewss提供了一个更高层次的抽象。[23]假设一个组织正在为网络内容提供服务，并且正在使用或愿意采用一个通用的服务器框架来处理HTTP请求和提供响应。框架的关键特征是控制权的倒置，因此，用户只负责编写某种 "行动 "或 "处理程序"--所选语言中接收请求参数并返回响应的函数。

在Borg的世界里，你运行这段代码的方式是建立副本的容器，每个副本包含一个由框架和用户代码组成的服务端。如果流量增加，你将通过扩大规模来处理（增加副本或扩展到新的数据中心）。如果流量减少，你将缩小规模。需要注意设置一个最小的规模。谷歌通常假设服务器运行的每个数据中心至少有三个副本。

但是，如果多个不同的团队使用同一个框架，就可以采用不同的方法：不只是多租户使用机器，我们也可以多租户使用同一服务端。在这种方法中，我们最终会运行更多的服务端，根据需要在不同的服务端上动态加载/卸载动作代码，并将请求动态地引导到那些加载了相关代码的服务端。各个团队不再运行服务器，这就是“Serverless”。

大多数关于Serverless框架的讨论都将其与“虚拟机即宠物”的模式相比较。在这种情况下，Serverless概念是一场真正的革命，因为它带来了“牛群”管理的所有好处：自动缩放、更低的开销、不需要明确的服务器。然而，正如前文所述，对于计划扩展的组织来说，转向共享、多租户、基于“牛”的模式应该已经是一个目标；因此，Serverless架构自然的比较点应该是“持久性容器”架构，如Borg、Kubernetes或Mesosphere。

#### 优点和缺点

首先要注意的是，Serverless架构要求你的代码是真正无状态的；我们不太可能在Serverless架构内运行用户的虚拟机或实现Spanner。我们之前谈到的所有管理本地状态的方法都不适用，除非不适用状态管理。在容器化的世界里，你可能会在启动时花几秒钟或几分钟的时间来设置与其他服务的连接，从冷存储中填充缓存，等等。在典型的情况下，你会在程序终止前得到一个宽限期。在Serverless模型中，不存在真正跨请求持久化的本地状态；所有你想使用的东西，都应该在请求范围内设置。

在实践中，大多数组织的需求都无法由真正的无状态工作负载来满足。这可能会导致依赖特定的解决方案（无论是本土的还是第三方的）来解决特定的问题（比如管理数据库的解决方案，这是公有云Serverless产品的常见配套），或者拥有两个解决方案：一个基于容器的解决方案和一个Serverless的解决方案。值得一提的是，许多或大多数Serverless框架建立在其他计算层之上。AppEngine运行在Borg上，Knative运行在Kubernetes上，Lambda运行在Amazon EC2上。

托管的Serverless模式对于资源成本的适应性扩展很有吸引力，特别是在低流量的一端。在比如说Kubernetes中，你的副本容器不能扩展到零容器（因为假设在请求服务时间内同时启动一个容器和一个节点太慢）。这意味着，在持久化集群模型中，仅仅拥有一个应用程序是有最低成本的。另一方面，Serverless应用程序可以很容易地扩展到零；因此，它的成本随着流量的增加而增加。

在非常高的流量端，你将必然受到底层基础设施的限制，不管采用什么计算解决方案。如果你的应用程序需要使用100,000个核心来服务流量，那么在你所使用的基础设施的物理设备中需要有100,000个物理核心可用。在较低端的情况下，如果你的应用有足够的流量让多个服务器忙碌，但又不足以给基础设施提供商带来问题，那么持久化容器解决方案和Serverless解决方案都可以扩展来处理，尽管Serverless解决方案的扩展将比持久化容器解决方案更加反应灵敏，更加细化。

最后，采用Serverless解决方案意味着在一定程度上失去了对环境的控制。在某种程度上，这是一件好事：拥有控制权意味着必须行使它，而这意味着管理上的开销。但当然，这也意味着，如果你需要一些所使用的框架中没有的额外功能，这将成为你的一个问题。

举个具体的例子，谷歌Code Jam团队（为数千名参赛者举办的编程比赛，其前端运行在谷歌AppEngine上）有一个定制的脚本，在比赛开始前几分钟给比赛网页带来了人为的流量高峰，以便为应用程序的足够实例预热，为比赛开始时的实际流量提供服务。这很有效，但这是人们希望通过选择Serverless解决方案来摆脱的那种手工调整（也是hacking行为）。

#### 权衡

谷歌在这种权衡中的选择是不对Serverless的解决方案进行大量投资。谷歌的持久化容器解决方案Borg足够先进，可以提供大部分Serverless的好处（比如自动伸缩、支持不同应用类型的各种框架、支持不同类型的部署工具、统一的日志和监控工具等等）。缺少的是更激进的扩展（特别是将规模缩小到零的能力），但谷歌的绝大部分资源需求来自高流量服务，因此过度配置小服务的成本相对较低。同时，谷歌运行着多个在“真正无状态”的世界中无法运行的应用程序，从GCE，到自制的数据库系统，如BigQuery或Spanner，到需要长时间填充缓存的服务器，如上述的长尾搜索服务工作。因此，对所有这些事情采用一个共同的统一架构的好处超过了对一部分工作负载采用单独的Serverless堆栈的潜在收益。

然而，谷歌的选择并不一定是每个组织的正确选择：其他组织已经成功地建立了混合容器/Serverless架构，或在纯粹的Serverless架构上利用第三方解决方案进行存储。

然而，Serverless的主要推力不是来自于大型组织，而是来自于小型组织或团队；在这种情况下，这种比较本身就是不公平的。Serverless模式虽然限制更多，但它允许基础设施供应商承担更大份额的整体管理费用，从而减少用户的管理费用。如果集群不被许多团队共享，在共享的Serverless架构上，如AWS Lambda或谷歌的Cloud Run，运行一个团队的代码，明显比建立集群以在GKE或AKS等托管的容器服务上运行代码更简单、更便宜。如果你的团队想获得托管计算服务的好处，但你的大型组织不愿意或无法转移到基于持久性容器的解决方案，那么公共云供应商的Serverless产品可能对你有吸引力，因为只有当集群真正在组织中的多个团队之间被共享时，共享集群的成本（包括资源和管理）才会摊薄。

然而，请注意，随着企业的发展和管理技术的普及，你很可能会超越纯粹的Serverless解决方案的限制。这使得存在突破路径的解决方案（比如从KNative到Kubernetes）很有吸引力，因为如果你的组织决定走这条路的话，它们提供了一条通往像谷歌这样的统一计算架构的自然路径。

### 公共还是私有

早在谷歌建立的时候，CaaS产品主要是自制的；如果想要一个，就得建造它。在公共还是私有这个问题上的的唯一选择是在拥有机器和租用机器之间，但是集群的管理都由自己决定。

在公有云时代，有更便宜的选择，但也有更多的选择，而一个组织将不得不做出选择。

使用公共云的机构实际上是将管理开销（部分）外包给公共云供应商。对于许多组织来说，这是一个有吸引力的提议--他们可以专注于在其特定的专业领域提供价值，不需要额外学习大量的基础设施专业知识。虽然云供应商（当然）收取的费用超过了原始服务器的最低成本，以收回管理费用，但他们已经建立了专业知识，并在多个客户之间共享这种知识。

此外，公共云是一种更容易扩展基础设施的方式。随着抽象水平的提高--从主机托管，到购买虚拟机时间，再到管理容器和Serverless产品--扩展的难度也在增加--从必须签署主机托管空间的租赁协议，到需要运行CLI来获得更多的虚拟机，再到自动扩展工具，你的资源占用随着你收到的流量而自动变化。特别是对于年轻的组织或产品，预测资源需求是具有挑战性的，因此，不必预先配置资源的优势是非常显著的。

在选择云计算供应商时，一个重要的顾虑是担心被锁定--供应商可能会突然涨价，或者直接倒闭，让企业陷入非常困难的境地。最早的Serverless提供商之一Zimki，一个运行JavaScript的平台即服务环境，在2007年关闭，只提前三个月通知。

对此的部分缓解措施是使用使用开源架构（如Kubernetes）运行的公共云解决方案。这是为了确保存在一个迁移路径，即使特定的基础设施供应商由于某种原因变得不可接受。虽然这减轻了很大一部分风险，但这并不是一个完美的策略。由于海勒姆定律，很难保证不使用特定于供应商的部分功能。

该策略有两个延伸的可能性。一种是使用较低层次的公共云解决方案（如亚马逊EC2），并在其上运行较高层次的开源解决方案（如OpenWhisk或KNative）。这试图确保如果你想迁移出去，你可以带着你对高级解决方案所做的任何调整，在它上面建立的工具，以及拥有的隐性依赖。另一种是使用多云；也就是说，使用基于两个或多个不同的云供应商的相同开源解决方案的管理服务（例如，Kubernetes的GKE和AKS）。这为迁移出其中一个提供了更容易的路径，同时也使你更难依赖其中某一个的具体实施细节。

还有一个相关的策略--不是为了解决锁定在某个特定云服务厂商的问题，而是为了管理迁移从而在混合云中运行；也就是说，在你的私有基础设施上有一部分整体工作负载，而在公共云供应商上运行一部分。其中一个方法是将公共云作为处理溢出的一种方式。一个组织可以在私有云上运行其大部分典型的工作负载，但在资源短缺的情况下，将一些工作负载扩展到公共云上。同样，为了使其有效运作，需要在两个空间使用相同的开源计算基础设施解决方案。

多云和混合云战略都需要通过不同环境中的机器之间的直接网络连接和两个环境中都有的通用API将多个环境很好地连接起来。

## 总结

在建设、完善和运行其计算基础设施的过程中，谷歌了解到设计良好的通用计算基础设施的价值。整个组织拥有一个单一的基础设施（例如，每个区域有一个或少量的共享Kubernetes集群），在管理和资源成本方面有显著的效率提升，并允许在该基础设施之上开发共享工具。在构建这样的架构时，容器是一个关键工具，允许在不同的任务之间共享一个物理或虚拟机器（导致资源效率的提高），以及在应用程序和操作系统之间提供一个抽象层，提供长期的弹性。

要很好地利用基于容器的架构，需要设计应用程序来使用“牛”模型：将你的应用程序设计成由可以轻松自动替换的节点组成，从而可以扩展到成千上万的实例。编写与该模型兼容的软件需要不同的思维模式；例如，将所有本地存储（包括磁盘）视为短暂的，避免硬编码主机名。

也就是说，尽管谷歌总体上对其架构的选择感到满意和成功，但其他组织将从广泛的计算服务中进行选择--从手工管理的虚拟机或机器的“宠物”模式，经过“牛”副本容器，到抽象的Serverless模式，都有管理和开源的味道；你的选择是对许多因素的复杂权衡。

## TL;DRs

* 规模化需要一个共同的基础设施来运行生产中的工作负载。

* 一个计算解决方案可以为软件提供一个标准化的、稳定的抽象和环境。

* 软件需要适应分布式、托管的计算环境。

* 一个组织的计算解决方案应该被深思熟虑地选择，以提供适当的抽象层次。

[1] 免责声明：对于某些应用，“运行它的硬件”是你的客户的硬件（例如，想想你十年前买的收缩包装的游戏）。这带来了非常不同的挑战，我们在本章中没有涉及。

[2] Abhishek Verma, Luis Pedrosa, Madhukar R Korupolu, David Oppenheimer, Eric Tune, and John Wilkes, “Large-scale cluster management at Google with Borg,” EuroSys, Article No.: 18 (April 2015): 1–17.

[3] 请注意，如果你的组织从公共云提供商那里租用机器，那么这一点和下一点就不太适用。

[4] 谷歌很早以前就这样选择了，由于数据换出到磁盘导致的延迟上升是如此可怕，以至于直接杀死进程并迁移到其它机器是普遍可取的，所以在谷歌的情况下，总是选择在内存耗尽时杀死进程。

[5] 尽管在减少这种开销方面正在进行大量的研究，但它永远不会像原生运行的进程那样低。

[6] 调度器并不能任意地这样做，而是出于具体的原因（比如需要更新内核，或者机器上的磁盘坏了，或者重新分配任务以使数据中心中工作负载的整体分布更平衡）。然而，拥有计算服务的意义在于，作为一个软件作者，我既不应该知道也不应该关心这些事情发生的原因。

[7] “宠物与牛”的比喻是Bill Baker向Randy Bias提出的，它作为描述“副本软件单元”概念的一种方式，已经变得非常流行。作为一种比喻，它也可以用来描述服务器以外的概念；例如，见第22章。

[8] 像所有的分类法一样，这个分类法并不完美；有一些程序类型并不适合任何一个类别，或者拥有服务和批处理工作的典型特征。然而，像大多数有用的分类法一样，它仍然捕捉到了许多现实生活中的区别。

[9] 参见 Jeffrey Dean and Sanjay Ghemawat, “MapReduce: Simplified Data Processing on Large Clusters,” 6th Symposium on Operating System Design and Implementation (OSDI), 2004.

[10] Craig Chambers, Ashish Raniwala, Frances Perry, Stephen Adams, Robert Henry, Robert Bradshaw, and Nathan Weizenbaum, “Flume‐Java: Easy, Efficient Data-Parallel Pipelines,” ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 2010.

[11] 参见 Atul Adya et al. “Auto-sharding for datacenter applications,” OSDI, 2019; and Atul Adya, Daniel Myers, Henry Qin, and Robert Grandl, “Fast key-value stores: An idea whose time has come and gone,” HotOS XVII, 2019.

[12] 请注意，除了分布式状态，建立一个有效的“服务器即牛”解决方案还有其他要求，比如服务发现和负载平衡系统（以便你的应用程序，在数据中心内移动，从而可以被有效访问）。因为这本书与其说是关于建立一个完整的CaaS基础设施，不如说是关于这样的基础设施与软件工程艺术的关系，所以我们在这里就不多说了。

[13] 例如参见 Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung, “The Google File System,” Proceedings of the 19th ACM Symposium on Operating Systems, 2003; Fay Chang et al., “Bigtable: A Distributed Storage System for Structured Data,” 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI); or James C. Corbett et al., “Spanner: Google’s Globally Distributed Database,” OSDI, 2012.

[14] 请注意，重试需要正确地实现--用优雅的降级和工具来避免像抖动这样的级联故障。因此，这可能应该是远程过程调用库的一部分，而不是由每个开发人员手工实现。例如，见SRE书中的第22章：解决级联故障。

[15] 这种情况在谷歌发生过多次；例如，因为有人在休假时留下了占用一千台谷歌计算引擎虚拟机的负载测试基础设施，或者因为一个新员工在他们的工作站上调试一个主二进制文件，而没有意识到它在后台产生了8000工作进程。

[16] 正如任何复杂的系统一样，也有例外。并非所有谷歌拥有的机器都由Borg管理，也不是每个数据中心都由一个Borg单元覆盖。但大多数工程师的工作环境是，他们不接触非Borg机器，也不接触非标准的单元。

[17] 这个特殊的命令在Borg下有害的，因为它阻止了Borg处理故障的机制的启动。然而，更复杂的包装器，例如将环境的一部分回显到日志中，仍然被用来帮助调试启动问题。

[18] 我的邮件服务器与你的图形渲染工作是不能互换的，即使这两个任务是在同一形式的虚拟机中运行。

[19] 这并不是使用户虚拟机可以实时迁移的唯一动机；它还提供了相当大的面向用户的好处，因为它意味着可以在不中断虚拟机的情况下对主机操作系统进行修补和主机硬件的更新。另一种方法（其他主要云供应商使用）是提供 "维护事件通知"，这意味着虚拟机可以，例如，重新启动或停止，随后由云供应商启动。

[20] 考虑到并非所有客户的虚拟机都选择实时迁移，这一点尤其重要；对于一些工作负载来说，即使是迁移过程中短时间的性能下降也是不可接受的。这些客户将收到维护事件通知，除非绝对必要，Borg将避免驱逐带有这些虚拟机的容器。

[21] 一个很好的提醒，监测和跟踪你的功能的使用情况是有价值的，尤其是随着时间的推移。

[22] 这意味着Kubernetes受益于清理Borg的经验，但一开始就没有受到广泛的现有用户群的阻碍，从一开始就在相当多的方面（比如它对标签的处理）明显更现代化。但是，Kubernetes现在也遇到了一些同样的问题，因为它已经在各种类型的应用中被广泛采用。

[23] FaaS（函数即服务）和PaaS（平台即服务）是与Serverless相关的术语。这三个术语之间有区别，但更多的是相似之处，而且界限有些模糊不清。
