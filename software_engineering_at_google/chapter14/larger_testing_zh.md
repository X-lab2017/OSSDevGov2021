# 第十四章
## 更大的测试						
															约瑟夫·格雷夫斯撰写，汤姆·曼施莱克编辑

在前面的章节中，我们已经叙述谷歌建立了属于自己的测试文化，以及单元测试是如何成为开发人员工作流程的基本部分的。但是其他类型的测试呢?事实证明，谷歌确实使用了许多其他的比单元测试大的测试，这些测试组成了软件编程过程中重要部分，用来应对风险。为了确保它们是有价值的资产，而不是资源的消耗这成为了测试的新挑战。在这一章中，我们将讨论 “大型测试”，什么是 “大型测试”，以及保持它们有效的最佳实践。

## 什么是“更大的测试”？
  如前所述，谷歌对测试大小是有定义的。小型测试仅限于一个线程、一个进程和一台机器。大型测试没有这种的限制。但是谷歌对测试范围也有概念定义。单元测试必须比集成测试的范围小。而最大范围的测试(有时称为端到端或系统测试)通常涉及几个真正的依赖项和更少的测试替身。


大型测试包含许多小测试所没有的东西。它们不受相同的约束;因此，它们可以表现出以下特征:  

•他们可能比较慢。我们的大型测试有一个默认15分钟或1小时的超时，但是我们也有运行几个小时甚至几天的测试。  
•它们可能是非封装好的。大型测试可能与其他测试和流量共享资源。  
•他们可能是不确定的。如果一个大型测试是非封装好的，它几乎是不可能保证决定论的:也就是说其他测试或用户状态可能会干扰它。  


那么为什么要进行更大的测试呢?回想一下你怎么敲代码的。你如何保证你写的程序可以正常运行?您可能正在写和运行单元测试，但你是否发现是你自己在尝试运行这些二进制软件呢?当你与他人分享这些代码时，他们如何测试它?让他们自己尝试运行单元测试?

另外，你怎么知道在版本迭代过程种你的代码还能正常运行?假设你有一个网站用到谷歌的地图API，同时这个API有待更新。你可能并不知道测试能否和新版本兼容所以你会尝试看一下运行是否正常。

## 保真度

使用较大测试的主要原因是为了解决保真度问题。保真度是一种属性，通过它，测试可以反映在测试过程种系统的真实情况（SUT）。

设想保真度是一种根据环境的方法。正如图14-1所示，单元测试将一个测试和一小部分代码捆绑在一起作为一个可运行的单元，这确保了代码经过了测试，但不同于生产代码的运行方式。自然，生产本身就是测试中保真度最高的环境。还有一系列临时选择权。大型测试的关键是找到合适的匹配，因为提高保真度也伴随着成本的增加和(在生产环境下)失败风险的增加。

![图14-1 保真度增加的规模](https://img-blog.csdnimg.cn/20210421160629827.png)
图14-1保真度增加的规模

还可以根据测试内容对现实的逼真度来衡量测试。 许多手工的，很大的测试都被工程师忽略，是因为这些测试都看起来不真实。 从生产中复制的测试数据更加接近于现实，但是最大的挑战是如何在之启动新代码之前创建真实的测试流量。 这在人工智能（AI）中也是个问题，因此，“种子”数据经常遭受内在偏见。并且因为大部分用来单元测试的数据都是手动生成的，这些数据只包含了例子的一小部分，而且往往带别了创造数据这个人的想法。没有被测试数据覆盖到的真实场景表明了一次测试的保真度差距。

## 单元测试中的常见差距
如果较小的测试失败，则可能需要较大的测试。 遵循的小节低表示某些特定区域中的单元测试无法提供良好的风险评估位置覆盖率。紧跟其后的分段只展现了一些特定的地方，因而单元测试不能很好的提供风险缓冲覆盖。

### 不可靠的双重测试
单个单元测试通常涵盖一个类或模块。双重测试(如第13章提到的)经常被用来消除影响大的的或难以测试的依赖性。但是当这些依赖被替换时，替换物和被替换物之间可能会有分歧。

Google几乎所有的单元测试都是由编写该测试的同一位工程师编写的。当那些单元测试需要双倍时，而且是模拟的话，是由工程师来给模拟下定义和指定行为。但这位工程师通常不会写模拟，因为他们可能传达错误的信息。被测的单元与给定同伴之间的关系是一种行为契约，如果工程师错误地理解了实际行为，那么对契约的理解是无效的。

而且模拟显得陈旧。如果基于模拟的单元测试对于实际运用的作者而言不可见的话，或者实际应用改变的话， 没有任何提示可以显示该测试需要和变化同步。

请注意，正如第13章中提到的，如果团队为他们自己提供假冒的服务，这种担忧就会大大减轻。

### 配置问题
单元测试涵盖给定的二进制代码。但是，就执行方式而言，二进制文件通常不是完全自给自足的。二进制文件通常需要一定的部署配置或启动脚本。此外，真正的终端用户服务产品都有自己的配置文件或配置数据库。

如果这些文件存在问题，或者这些存储定义的状态与涉及的二进制文件之间存在兼容性问题，那么这些问题可能会导致重大的用户问题。单靠单元测试无法验证这种兼容性。另外说一句，这反而是一个让你保证配置和代码都处于版本控制中的不错理由。因为随后的配置变化可以被识别来作为bug（漏洞）的来源而不是引入外部的随机碎片并且这可以运用于大测试中。

*[单靠单元测试无法验证这种兼容性。]:   请参阅第483页和第25章的“持续交付”了解更多信息。


在谷歌，配置更改是我们主要停机的首要原因。这是我们表现不佳的一个领域，并导致了一些最令人尴尬的bug。例如，在2013年有一次谷歌的全球宕机，原因是一个糟糕的网络配置推送因为它未经测试。配置往往是用配置语言编写的，而不是生产环境的代码编写的。与二进制文件相比，它们的产品推出周期通常更快，而且可能更难被测试。所有这些都导致了失败的可能性。但至少在这种情况下(以及其他情况)，配置被包括进版本控制，然后我们可以很快的找到罪魁祸首并减轻问题的发生。

### 负载情况下的问题
在谷歌，单元测试的目的是要小而快，因为它们需要适应我们的标准测试的基础结构，而且还可以作为无摩擦开发人员工作流的一部分而多次运行。但是性能、负载和压力测试通常需要向给定的二进制文件发送大量的通信。这么大的量在普通的单元测试模型中很难进行。而且我们的访问量非常大，通常每秒有数千或数百万次查询(比如广告和实时竞价)！

### 未预料到的行为、输入和副作用
单元测试是受限于工程师的想象力的。也就是说，它们只能测试预期的行为和输入。然而，用户在产品中发现的问题大多是意料之外的(否则，它们不太可能作为问题出现在最终用户面前)。这一事实表明，需要不同的测试技术来测试未预料到的行为。
Hyrum法则在这里是一个重要的考虑因素:即使我们可以100%地测试一致性基于严格的具体的协议，有效的用户合同适用于所有可见的行为，而不仅仅是一个规定的合同。单元测试不太可能单独测试公共API中没有指定的所有可见行为。

### 突发行为与“真空效应”
单元测试被限制在它们所覆盖的范围内(特别是在广泛使用双重测试的情况下)，因此，如果行为在这个范围之外的区域发生了变化，则无法检测到它。而且，由于单元测试被设计成快速和可靠的，它们有意地消除了实际依赖关系、网络和数据的混乱。单元测试就像理论物理学中的问题:放在真空中，巧妙地隐藏在现实世界的混乱之中，这对速度和可靠性很有好处，但却忽略了某些缺陷。

## 为什么不进行更大的测试呢？
在前面的章节中，我们讨论了开发人员友好测试的许多属性。
特别地，它需要如下：

可靠的
它一定不能剥落，并且必须提供有用的通过/失败信号。

快速地:
它需要足够快以不打扰开发人员的工作流程。

可扩展:
Google需要能够有效地运行所有这些有用的受影响的测试，以进行预sub-麻省理工学院和提交后。好的单元测试具有所有这些特性。较大的测试通常会违反所有这些条件约束。例如，较大的测试通常比较脆弱，因为它们使用更多的红外线结构要比小型单元测试好。它们通常也慢得多，两者都需要设置以及跑步。而且由于资源和时间的原因，他们很难扩展要求，但通常还因为它们不是孤立的—这些测试可能会发生冲突彼此之间。

另外，较大的测试提出了另外两个挑战。首先，有一个挑战所有权。单元测试显然由拥有该测试的工程师（和团队）拥有单元。较大的测试跨多个单元，因此可以跨多个所有者。这提出了长期所有权挑战：谁负责维护测试当测试中断时，谁负责诊断问题？没有明确所有权，考验腐烂。

大型测试的第二个挑战是标准化（或缺乏标准化）之一。
与单元测试不同，大型测试的基础设施缺乏标准化编写，运行和调试它们的结构和过程。的方法大型测试是系统架构决定的产物，因此引入了var-要求的测试类型。例如，我们构建和运行A-B差异的方式Google Ads中的回归测试与此类测试的方式完全不同在搜索后端中构建并运行，这与云端硬盘有所不同。他们使用不同的功能强大的平台，不同的语言，不同的基础架构，不同的库，以及竞争性测试框架。

缺乏标准化会产生重大影响。 因为较大的测试有有许多种运行方式，在大规模更改中通常会跳过它们。 （看第22章。基础结构没有运行这些测试的标准方法，并且要求执行LSC的人员了解每个测试的本地详细信息团队规模不大。 由于大型测试的实施因团队而异，实际测试这些团队之间的集成的测试需要统一的不兼容可行的基础架构。也因为缺少相应的标准化流程，我们不能教对Nooglers(新谷歌人)或更有经验的工程师采取单一的方法，这既使这种情况持续下去，也导致人们对此类测试的动机缺乏理解。

## 在谷歌更大的测试
当我们之前讨论谷歌的测试历史时(见第11章)，我们提到了谷歌网络服务 (GWS)是如何在2003年强制进行自动化测试的，以及这是如何成为一个分水岭的。然而，在此之前，我们实际上已经使用了自动化测试，但常见的做法是使用自动化的大型测试。例如，AdWords在2001年创建了一个端到端测试，以验证产品的实际情况。类似地，2002年，Search为它的索引代码编写了一个类似的“回归测试”，而AdSense(它甚至还没有公开发布)在AdWord上创建了它的变体。

其他“更大”的测试模式大约也存在于2002年。谷歌搜索前端严重依赖端到端测试场景的手动qa -手动版本。和Gmail获得了它的“本地演示”环境版本——一个脚本，用来在本地创建一个端到端Gmail环境，并生成一些测试用户和邮件数据，用于本地手动测试。

当C/J构建(我们的第一个持续构建框架)发布时，它并没有区分单元测试和其他测试，但是有两个关键的开发导致了分裂。
首先，谷歌专注于单元测试，因为我们希望鼓励测试金字塔，并确保绝大多数的书面测试都是单元测试。
第二，当TAP取代C/J Build作为我们正式的持续构建系统时，它只能对满足TAP合格性要求的测试这样做:可在单个更改下构建的密封测试，该更改可以在最大时间限制内运行在我们的构建/测试集群上。
虽然大多数单元测试满足这一要求，但大型测试大多不满足这一要求。
然而，这并没有阻止对其他类型的测试的需要，它们继续填补覆盖范围的空白。
C/J Build甚至为了处理这类测试而存在了好几年，直到更新的系统取代它。

### 更大的测试和更多的时间
在本书中，我们探讨了时间对软件工程师的影响因为谷歌开发的软件已经运行了20多年。是如何受时间维度影响较大的测试?我们知道某些活动会代码的预期寿命越长，对各种形式的测试就越有意义在所有级别上都有意义的活动，但是测试类型是适当的在代码的预期生命周期内。

正如我们之前指出的，单元测试开始对具有预期寿命数小时以上的软件是具有意义的。在分钟级别上，手动测试是非常普遍的。往往STU在本地测试。但本地的演示可能是生产，特别是一次性脚本、演示或实验。在较长的寿命期间，手工测试将继续存在，但SUT通常会出现分歧，因为生产实例通常是云托管的，而不是本地托管的。

其余的大型测试都为长期存在的软件提供了价值，但是随着时间的增加，主要的关注点变成了此类测试的可维护性。

顺便说一下，这种时间影响可能是开发“冰淇淋甜筒”测试反模式的原因之一，如第11章所述，以及这里的图14-2所表示的。
![图14-2](https://img-blog.csdnimg.cn/20210425103209828.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgyMzAxOQ==,size_16,color_FFFFFF,t_70)
图14-2

  当开发开始于手动测试时(当工程师认为代码只会持续几分钟)，这些手动测试就会累积起来，并主导整个测试组合。例如，它是相当典型的入侵一个脚本或应用程序，并通过运行它来测试它，然后继续添加功能，但继续通过手动运行它来测试它。这个原型最终会实现功能，并与他人共享，但实际上没有针对它的自动化测试。

糟糕的是，如果代码很难进行单元测试(因为它最初的实现方式)，那么唯一可以编写的自动化测试就是端到端测试，而我们在几天内就无意中创建了“遗留代码”。

在开发的头几天内通过构建单元测试向测试金字塔过渡，然后通过引入自动化集成测试和从手动的端到端测试转移到测试金字塔，这对于长期健康是至关重要的。我们成功得将单元测试作为一次请求提交了，但是覆盖单元测试和手动测试之间的差距对于长期健康运行是必要的。

### 谷歌规模的更大测试
在更大的软件规模中，似乎更大的测试更有必要也更合适，但即便如此，编写、运行、维护和调试这些测试的复杂性也会随着规模的增长而增加，甚至比单元测试更复杂。

微服务组成的一个系统或单独的服务器,公司的模式及互连附带应承担的看起来像一个图:让图中的节点数量是我们的n .每次一个新节点添加到这张图,有一个乘法效应的数量不同的执行路径。

图14-3描绘了一个想象中的SUT:这个系统由一个有用户的社交网络、一个社交图、一串帖子和一些广告组成。广告是由广告商创造的，并在社交流的背景下服务。这个SUT单独由两组用户、两个ui、三个数据库、一个索引管道和六个服务器组成。
图中枚举了14条边。测试所有的端到端可能性已经很困难了。想象一下，如果我们向这个组合中添加更多的服务、管道和数据库:照片和图像、机器学习照片分析等等?

![图14-3](https://img-blog.csdnimg.cn/20210425111945182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgyMzAxOQ==,size_16,color_FFFFFF,t_70)图14-3 一个相当小的SUT的例子:一个带有广告的社交网络

以端到端方式进行测试的不同场景的速率可以呈指数级增长或组合增长，这取决于被测试系统的结构，而且这种增长不具有伸缩性。因此，随着系统的增长，我们必须找到可选择的更大的测试策略来保持可管理。

然而，由于为实现这一比例尺而作出的必要决定，这种检验的价值也有所增加。
这是保真度的影响:
当我们迈向大N层软件,如果服务双打低保真(1 -ε),错误的机会把它放在一起时指数n .再看看这个例子SUT,如果我们将用户服务器和广告服务器替换为双打和混双低保真(例如,10%准确),
出现bug的可能性为99%(1(0.1 0.1))。这还只是两个低保真度。

因此，以在此规模上工作良好但保持合理高保真度的方式实现更大的测试变得至关重要。

![图14-4](https://img-blog.csdnimg.cn/20210425112212624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgyMzAxOQ==,size_16,color_FFFFFF,t_70)
图14-4 链式测试

## 更大测试的结构


尽管大型测试不受小型测试约束的约束，并且可以由任何东西组成，但大多数大型测试都表现出常见的模式。大型测试通常包含以下阶段的工作流:  

•获得一个被测系统  
•播种必要的试验数据  
•使用被测系统执行操作  
•验证行为  


## 被测系统
大型测试的一个关键组成部分就是前述的SUT（见图14-5）。 典型的单元测试将注意力集中在一个类或模块上。 而且，测试代码与被测试的代码在相同的进程（或Java情况下为Java虚拟机[JVM]）中运行。 对于较大的测试，SUT通常非常不同。 一个或多个带有测试代码的单独进程经常（但不总是）在其自己的进程中。
*图14-5.被测系统示例*
在Google，我们使用许多不同形式的SUT，SUT的范围是大型测试本身范围的主要驱动力之一（SUT越大，测试越大）。 可以根据两个主要因素来判断每种SUT形式：
*气密性*
这是SUT与正在使用的产品以及与所测试以外的其他组件之间的交互作用的隔离。 具有高度气密性的SUT暴露于并发源和基础架构脆弱性的可能性最小。
*保真度*
SUT反映测试中的生产系统的准确性。 具有高保真度的SUT将包含类似于生产版本的二进制形式（依赖于相似的配置，使用相似的基础结构并且具有相似的整体拓扑）。
通常来说这两个因素会有矛盾。
以下是部分SUT的实例：
*单进程SUT*
整个被测系统都打包为一个二进制文件（即使在生产中它们是多个单独的二进制文件）。 此外，可以将测试代码打包到与SUT相同的二进制文件中。 如果所有内容都是单线程的，则这种测试-SUT组合可能是“小型”测试，但对生产拓扑和配置的可信度最低。
*单机SUT*
被测系统由一个或多个单独的二进制文件（与生产系统相同）组成，并且测试是其自己的二进制文件。 但是一切都在一台机器上运行。 这用于“中等”测试。 理想情况下，我们在本地运行这些二进制文件时使用每个二进制文件的生产启动配置，以提高保真度。
*多机SUT*
被测系统分布在多台机器上（非常像生产云部署）。 与单机SUT相比，它的保真度甚至更高，但是它的使用使测试变得“大”，并且这种组合容易受到网络和机器脆弱性的影响。
*共享环境（暂存和生产）*
该测试仅使用共享环境，而不是运行独立的SUT。这样做成本最低，因为这些共享环境通常已经存在，但是该测试可能与其他同时使用发生冲突，因此必须等待将代码推送到这些环境中。 。 生产还增加了最终用户影响的风险。
*混合情况*
一些SUT表示混合：可能可以运行某些SUT，但可以使其与共享环境交互。 通常，要测试的事物是显式运行的，但其后端是共享的。 对于一家像Google这样规模庞大的公司来说，实际上不可能运行所有Google互连服务的多个副本，因此需要某种混合。

### 密封式SUT的好处
大型测试中的SUT可能是不可靠和周转时间长的主要来源。例如，生产测试使用实际的生产系统部署。 如前所述，这是流行的，因为环境没有额外的开销成本，但是在代码到达该环境之前无法运行生产测试，这意味着这些测试本身无法阻止代码向该环境的发布，也即SUT为时已晚。
最常见的第一种选择是创建一个巨大的共享登台环境并在其中运行测试。 这通常是在某些发行促进过程中完成的，但它再次将测试执行限制为仅在代码可用时执行。作为替代方案，一些团队将允许工程师在临时环境中“保留”时间，并使用该时间窗口来部署待处理的代码并运行测试，但这并不能随着越来越多的工程师或越来越多的服务而扩展 ，因为环境，其用户数量以及用户冲突的可能性都在迅速增长。
下一步是支持云隔离的或机器密封的SUT。这样的环境通过避免冲突和代码发布的保留要求来改善情况。
***
## 案例研究：在生产和Webdriver Torso中进行测试的风险
我们提到在生产中进行测试可能会带来风险。生产中的测试导致的一个幽默事件被称为Webdriver Torso事件。我们需要一种方法来验证YouTube产品中的视频渲染是否正常地创建了自动脚本来生成测试视频，上传它们并验证上传的质量 这是在Google拥有的YouTube频道Webdriver Torso中完成的。 但是这个频道和大多数视频都是公开的。
随后，该频道在《连线》上的一篇文章中进行了宣传，导致该频道在媒体中广泛传播，并为解决这个谜团做出了后续努力。 最后，博主将所有内容都归还给Google。最终，我们通过玩一些有趣的游戏而变得干净，包括Rickroll和Easter Egg，因此一切工作都很好。但是我们确实需要考虑最终用户发现我们生产中包含的任何测试数据并为此做好准备的可能性。
***
### 在边界减小SUT大小
有一些特别不友好的测试边界可以避免。涉及前端和后端的测试十分痛苦，因为众所周知，用户界面（UI）测试不可靠且成本很高。
* UI经常变化外观，这使UI测试变难，但实际上并没有影响基础行为。
* UI通常具有难以测试的异步行为。 

尽管对服务的UI进行端到端测试一直到后端都很有用，但是这些测试对UI和后端都有成倍的维护成本。 相反，如果后端提供了公共API，则通常更容易在UI / API边界将测试分为连接的测试，并使用公共API驱动端到端测试。 无论UI是浏览器，命令行界面（CLI），桌面应用程序还是移动应用程序，这都是正确的。
第三方依赖项是另一个特殊的边界。 第三方系统可能没有用于测试的公共共享环境，在某些情况下，将流量发送给第三方会产生一定的成本。 因此，不建议让自动化测试使用真正的第三方API，并且这种依赖关系是拆分测试的重要缝隙。
为了解决此大小问题，我们通过用内存数据库替换其数据库并删除了我们实际关心的SUT范围之外的其中一台服务器，从而使该SUT变得更小，如图14-6所示。 该SUT更有可能安装在一台机器上。
*图14-6.SUT裁剪版*
关键在于确定保真度与成本/可靠性之间的权衡，并确定合理的界限。 如果我们可以运行少量的二进制文件并进行测试，然后将其全部打包到执行常规编译，链接和单元测试执行的同一台计算机中，那么对于我们的工程师而言，我们将拥有最简单，最稳定的“集成”测试。
### 记录/重播的代理
在上一章中，我们讨论了测试加倍和方法，这些方法可用于将被测类与其难以测试的依赖项分离开来。我们还可以通过使用具有等效API的模拟，存根或伪造服务器或进程来使整个服务器和进程加倍。但是，不能保证所使用的测试倍数实际上符合它要替换的真实物品的协议。
处理SUT的依存于附属服务的一种方法是使用双重测试，但人们如何知道双重反映依存关系的实际行为呢？ 在Google之外，一种日益增长的方法是使用一个框架来进行消费者驱动的合同测试。这些是为客户和服务提供者定义合同的测试，并且该合同可以推动自动化测试。也就是说，客户端定义了服务的模拟，也即对于这些输入参数，我得到特定的输出。然后，真实服务在真实测试中使用该输入/输出对，以确保在给定这些输入的情况下产生该输出。消费者驱动的合同测试的两个公共工具是契约合同测试和Spring Cloud合同。Google严重依赖协议缓冲区，这意味着我们不在内部使用这些缓冲区。
在Google，我们做了一些与众不同的事情。最受欢迎的方法（有一个公共API）是使用较大的测试来生成较小的测试，即在运行较大的测试时记录到这些外部服务的流量，并在运行较小的测试时重放这些流量。较大的测试或“记录模式”测试在提交后连续运行，但是其主要目的是生成这些流量日志（必须通过测试才能生成日志）。在开发和预提交测试期间使用较小的或“重播模式”测试。
记录/重放的工作方式有趣的方面之一在于，由于不确定性，必须通过匹配器匹配请求以确定要重放的响应。这使得它们与存根和模拟非常相似，因为参数匹配用于确定结果行为。
在新的测试或客户端行为发生显著变化的测试中会发生什么情况？在这些情况下，请求可能不再与记录的流量文件中的内容匹配，因此测试无法在重播模式下通过。在这种情况下，工程师必须以“记录”模式运行测试以生成新的流量，因此使“运行”记录测试简易、快速和稳定是十分重要的。
## 测试数据
一个测试需要数据，而一个大型测试则需要两种不同的数据：
*种子数据*
预初始化到被测系统中的数据，反映了测试开始时SUT的状态。
*测试轨迹*
在测试执行期间由测试本身发送到被测系统的数据。
由于存在单独且更大的SUT的概念，因此，播种SUT状态的工作通常比在单元测试中完成的设置工作要复杂几个数量级。
例如：
*域数据*
一些数据库包含预先填充到表中并用作环境配置的数据。如果未提供域数据，则使用此类数据库的实际服务二进制文件可能会在启动时失败。
*现实的基线*
为了将SUT视为现实，可能会在启动时就质量和数量方面要求一组现实的基础数据。例如，社交网络的大型测试可能需要一个真实的社交图作为测试的基本状态：足够多的具有逼真的配置文件的测试用户以及这些用户之间必须存在足够的互连关系才能接受测试。
*播种APIs*
用来播种数据的API可能很复杂。 可能可以直接写入数据存储，但是这样做可能会绕过由执行写入操作的实际二进制文件执行的触发器和检查。
数据可以通过不同的方式生成，例如：
*手工数据*
像较小的测试一样，我们可以手动为较大的测试创建测试数据。 但是在大型SUT中为多个服务设置数据可能需要更多的工作，并且我们可能需要为大型测试创建大量数据。
*复制的数据*
我们可以复制数据，通常是从生产中复制数据。例如，我们可以通过从生产地图数据的副本开始提供基准来测试地球地图，然后测试对其所做的更改。
*样例数据*
复制数据可能会提供过多的数据，无法合理使用。采样数据可以减少数据量，从而减少测试时间并使推理更加容易。“智能采样”包括复制实现最大覆盖范围所需的最小数据的技术。
## 验证
在运行SUT并将流量发送到它之后，我们仍然必须验证行为，可以通过几种不同的方法来完成此操作：
*手工*
就像您在本地尝试二进制文件时一样，手动验证使用人工与SUT交互以确定其是否正常运行。该验证可以包括通过执行一致的测试计划中定义的操作来测试回归，也可以是探索性的，通过不同的交互路径来确定可能的新故障。
请注意，手动回归测试不能线性地扩展：系统越大，并且经过的行程越多，则需要更多的人工时间进行手动测试。
*断言*
就像单元测试一样，这些是对系统预期行为的显式检查。 例如，对于Google搜索xyzzy的集成测试，一个断言可能如下：
			assertThat(response.Contains("Colossal Cave"))
*A/B比较（差异）*
A / B测试不是定义显式的断言，而是运行SUT的两个副本，发送相同的数据并比较输出。没有明确定义预期的行为：人们必须手动检查差异以确保进行任何预期的更改。
## 大规模测试类型
现在，我们可以将这些不同的SUT，数据和断言方法结合起来，以创建不同种类的大型测试。 这样，每个测试就可以减轻哪些风险就具有不同的属性。 编写，维护和调试它需要多少工作； 以及在运行资源方面的成本。
以下是我们在Google上使用的各种大型测试的列表，它们的组成方式，服务的目的以及它们的局限性：
* 一个或多个二进制文件的功能测试
* 浏览器和设备测试
* 性能，负载和压力测试
* 部署配置测试
* 探索性测试
* A / B差异（回归）测试
* 用户验收测试（UAT）
* 探针和金丝雀分析
* 灾难恢复与混乱工程
* 用户评估


考虑到如此众多的组合，因此进行了广泛的测试，我们如何管理做什么以及何时进行？ 设计软件的一部分正在起草测试计划，而测试计划的关键部分是确定需要哪种类型的测试以及每种类型需要多少测试的战略要点。该测试策略确定了主要的风险向量以及减轻这些风险向量的必要测试方法。
在Google，我们担负着“测试工程师”的专门工程角色，而我们希望优秀的测试工程师所追求的目标之一就是能够为我们的产品制定测试策略。
## 一或多个交互二进制文件功能测试
这些类型的测试具有以下特征：
* SUT：单机密封或云部署隔离
* 数据：手工制作
* 验证：断言

到目前为止，我们已经看到，单元测试无法真正保真地测试复杂的系统，这仅仅是因为它们的打包方式与实际代码的打包方式不同。许多功能测试方案与给定的二进制文件进行交互的方式与该二进制文件内部的类的交互方式不同，并且这些功能测试需要单独的SUT，因此是规范的大型测试。
测试多个二进制文件的交互比测试单个二进制文件还要复杂的情况并不奇怪。当服务被部署为许多单独的二进制文件时，微服务环境中是一个常见的用例。在这种情况下，通过启动由所有相关二进制文件组成的SUT并通过已发布的API进行交互，功能测试可以涵盖二进制文件之间的实际交互。

## 浏览器和设备测试
测试Web UI和移动应用程序是对一个或多个交互二进制文件进行功能测试的一种特殊情况。可以对基础代码进行单元测试，但是对于最终用户而言，公共API是应用程序本身。使测试通过前端与作为第三方的应用程序进行交互可以提供额外的覆盖范围。
## 性能，负载和压力测试
这些类型的测试具有以下特征：
* SUT：云部署隔离
* 数据：来自生产的手工制作或多路复用
* 验证：差异（性能指标）


尽管可以在性能，负载和压力方面测试较小的单元，但此类测试通常需要同时向外部API发送流量。 该定义意味着此类测试是多线程测试，通常在被测二进制文件的范围内进行测试。但是，这些测试对于确保版本之间的性能不会降低以及系统可以处理预期的流量高峰至关重要。
随着负载测试规模的增长，输入数据的范围也随之增长，最终将难以生成触发负载下的错误所需的负载规模。负载和压力处理是系统的“高度紧急”特性。也就是说，这些复杂的行为属于整个系统，但不属于单个成员。因此，重要的是要使这些测试看起来尽可能地接近生产环境。每个SUT都需要与生产所需资源相似的资源，并且难以减轻生产拓扑中的噪声
消除性能测试中的噪声的一项研究领域是修改部署拓扑，即各种二进制文件如何在计算机网络中分布。运行二进制文件的计算机可能会影响性能特征；因此，如果在性能差异测试中，基本版本在快速计算机（或具有快速网络的计算机）上运行，而新版本在速度较慢的计算机上运行，则它看起来将会像是性能下降。此特征意味着最佳部署是在同一台计算机上运行两个版本。如果一台机器不能同时使用两个版本的二进制文件，则另一种方法是通过执行多次运行并消除峰和谷来进行校准。
## 部署配置测试
这些类型的测试具有以下特征：
* SUT：单机密封或云部署隔离
* 数据：无
* 验证：断言（不会崩溃）


很多时候，缺陷的来源不是代码，而是缺陷的配置：数据文件，数据库，选项定义等。 较大的测试可以测试SUT及其配置文件的集成，因为在启动给定的二进制文件期间会读取这些配置文件。这样的测试实际上是对SUT的冒烟测试，不需要太多的额外数据或验证。如果SUT成功启动，则测试通过。 如果不是，则测试失败。
## 探索性测试
这些类型的测试具有以下特征：
* SUT：生产或共享暂存
* 数据：生产或已知的测试领域
* 验证：手动


探索性测试是一种手动测试的形式，其重点不是通过重复已知的测试流程来寻找行为回归，而是通过尝试新的用户场景来寻找可疑行为。受过培训的用户/测试人员通过产品的公共API与产品进行交互，寻找通过系统的新路径，以及行为是否偏离预期或直观行为，或者是否存在安全漏洞。
探索性测试对于新系统和已发布系统都非常有用，以发现意外行为和副作用。 通过让测试人员遵循贯穿系统的不同可达路径，我们可以扩大系统覆盖范围，并在这些测试人员发现错误时捕获新的自动化功能测试。 从某种意义上讲，这有点像功能集成测试的手动“模糊测试”版本。
### 局限
手动测试不能线性扩展；也就是说，进行手动测试需要人工。探索性测试中发现的任何缺陷都应使用可以更频繁地运行的自动测试来复制。
### 漏洞
我们用于手动探索性测试的一种常见方法是bug bash。一组工程师和相关人员（经理，产品经理，测试工程师，任何熟悉产品的人员）安排了一次“会议”，但在本次会议上，每个参与人员都手动进行了产品测试。可能有一些针对特定漏洞领域和/或使用系统的出发点的已发布指南，但目标是提供足够的交互方式，以记录可疑的产品行为和彻底的错误。
## A / B差异回归测试
这些类型的测试具有以下特征：
* SUT：两个部署了云的隔离环境
* 数据：通常从生产中多路复用或采样
* 验证：A / B差异比较

单元测试涵盖了一小段代码的预期行为路径。但是，无法预测给定面向公众的产品的许多可能的故障模式。此外，正如Hyrum定律所指出的那样，实际的公共API并不是声明的一种，而是产品的所有用户可见的方面。考虑到这两个属性，A / B差异测试可能是Google进行大型测试的最常见形式，这不足为奇。从概念上讲，这种方法可以追溯到1998年。自2001年以来，我们在Google的大多数产品（从广告，搜索和地图开始）就一直基于此模型进行测试。
A / B差异测试通过将流量发送到公共API并比较新旧版本之间的响应（尤其是在迁移过程中）来进行操作。行为上的任何偏差都必须按照预期或未预期的方式进行调和（回归）。在这种情况下，SUT由两组实际二进制文件组成：一组运行在候选版本上，另一组运行在基本版本上。并且第三个二进制文件发送流量并比较结果。
当然也有其他变体。我们使用A-A测试（将系统与其自身进行比较）来识别不确定的行为，噪声和脆弱性，并帮助从A-B差异中删除那些不确定性。我们还偶尔使用ABC测试，比较最后的生产版本，基准构建和有待更改的内容。最后不仅可以看到立即更改的影响，还可以在下一个发行版本看到潜在更改的累积影响。
A / B差异测试是一种方便但可自动化的方法，用于检测已启动系统的任何异常情况。

### 局限

差异测试确实带来了一些要解决的挑战:  
*赞同*  
必须有人足够理解结果，才能知道于预期是否会有任何差异。 与典型的测试不同，其不清楚差异是好是坏（或基准版本实际上是否有效），因此在此过程中通常需要手动进行操作。  
*噪音*  
对于差异测试，将任何意外的噪声引入结果的方法都会导致对结果进行更多的人工调查。因此有必要弥补噪声，这是构建良好的差异测试的一大复杂原因。  
*覆盖范围*  
为差异测试生成足够的有用流量可能是一个具有挑战性的问题。 测试数据必须涵盖足够的场景以识别极端情况下的差异，但是手动整理此类数据非常困难。  
*设置*  
配置和维护一个SUT颇具挑战性。 一次创建两个可以使复杂度增加一倍，尤其是当它们共享相互依赖关系时。  

## UAV

这种类型的测试具有以下特征:
• SUT:机器密封或云部署隔离
• 数据:人工制造
• 验证:断言

单元测试的一个关键方面是它们由编写测试代码的开发人员编写。但这使得对产品预期行为的误解很可能不仅反映在代码中，也反映在单元测试中。这种单元测试验证代码是“按实现方式工作”，而不是“按预期方式工作”。

对于有特定最终客户或客户代理(客户委员会甚至产品经理)的情况，用户验收测试是通过公共应用程序接口对产品进行测试的自动化测试，以确保特定用户的整体行为符合预期。存在多种公共框架(例如，Cucumber和RSpec)，使得这样的测试可以用用户友好的语言进行写或读，通常在“可运行规范”的上下文中。

谷歌实际上没有做很多自动化的UAT，也不怎么使用规范语言。谷歌的许多产品在历史上都是由软件工程师自己创造的。几乎不需要可运行的规范语言，因为那些定义预期产品行为的语言本身经常是流畅的实际编码语言。

## 探针和金丝雀分析

这种类型的测试具有以下特征:
• SUT:真实生产环境
• 数据:真实环境中产生
• 验证:断言以及A/B差异（的度量指标）

探针和金丝雀分析是确保生产环境本身健康的方法。在这些方面，它们是生产监控的一种形式，但它们在结构上与其他大型测试非常相似。

探针是针对生产环境运行编码断言的功能测试。通常，这些测试执行众所周知的确定性只读操作，因此即使生产数据随着时间的推移而变化，断言仍然有效。例如，一个探测器可能在www.google.com进行谷歌搜索，并验证返回了一个结果，但实际上并没有验证结果的内容。在这方面，它们是生产系统的“烟雾测试”,但是它们提供了主要问题的早期检测。

金丝雀分析是相似的，除了它关注于一个版本何时被推送到生产环境。如果发布是分阶段进行的，我们可以运行两个针对升级的(金丝雀)服务探针断言，并比较canary和基线生产部分的健康度量，确保它们没有越界。

探针应该用在任何带电系统中。如果生产部署过程包括将二进制文件部署到生产机器的有限子集的阶段(金丝雀阶段)，金丝雀分析应该在该过程中使用。

### 限制

此时(生产中)发现的任何问题都已经影响到最终用户。

如果探测器执行一个可变的(写)动作，它将修改生产状态。这可能导致三种结果之一:断言的不确定性和失败，未来写能力的失败，或者用户可见的副作用。

## 灾难恢复和混沌工程

这种类型的测试具有以下特征:
• SUT:真实生产环境
• 数据:真实环境中产生以及用户定制(错误注入)
• 验证:人工验证以及A/B差异（的度量指标）

这些测试您的系统对意外变化或故障的反应。

多年来，谷歌一直在运行一个名为DiRT（灾难恢复测试）的年度战争游戏，在此期间，故障会以近乎全球的规模注入我们的基础设施。我们模拟从数据中心火灾到恶意攻击的一切。在一个可记忆的案例中，我们模拟了一场地震，这场地震将我们位于加州山景城的总部与公司的其他部门完全隔离开来。这样做不仅暴露了技术上的缺陷，也暴露了在所有关键决策者都无法联系到的情况下经营一家公司的挑战。

DiRT测试的影响需要整个公司的大量协调；相比之下，混沌工程更多的是对你的技术基础结构的“持续测试”。混沌工程因网飞（Netflix）而流行，它包括编写一个不断地在你的系统中引入背景级别的错误的程序，看看会发生什么。有些故障可能相当大，但在大多数情况下，混乱测试工具被设计成在事情失控之前恢复功能。混沌工程的目标是帮助团队打破稳定性和可靠性的假设，并帮助他们应对构建弹性的挑战。今天，Google的团队每周使用我们自己开发的名为Catzilla的系统进行数千次混沌测试。

这些类型的故障和阴性测试对于实际生产系统是有意义的，这些系统有足够的理论容错能力来支持它们，并且测试本身的成本和风险是可以承受的。

### 限制

此时(生产中)发现的任何问题都已经影响到最终用户。

DiRT是相当昂贵的运行，因此我们运行一个不经常规模的协调练习。当我们造成这种程度的停机时，我们实际上会造成痛苦并对员工绩效产生负面影响。

如果探测器执行一个可变的(写)动作，它将修改生产状态。这可能导致断言的不确定性和失败，未来写能力的失败，或者用户可见的副作用。

## 用户评估

这种类型的测试具有以下特征:
• SUT:真实生产环境
• 数据:真实环境中产生
• 验证:人工验证以及A/B差异（的度量指标）

基于生产的测试使得收集大量关于用户行为的数据成为可能。我们有几种不同的方法来收集关于即将推出的功能的受欢迎程度和问题的指标，这为我们提供了一种替代UAT的方法:

#### 狗粮

使用有限的实验来使生产中的特性对一部分用户可用是有可能的。我们有时会和自己的员工一起这样做(吃自己的狗粮)，他们会在实际部署环境中给我们有价值的反馈。

#### 实验

一种新的行为可以在用户不知情的情况下作为实验提供给用户子集。然后，将实验组与对照组在某个期望的度量方面在总体水平上进行比较。例如，在YouTube上，我们做了一个有限的实验来改变视频向上投票的方式(消除向下投票)，只有一部分用户看到了这种变化。

这对谷歌来说是一个非常重要的方法。Noogler在加入该公司时听到的第一个故事是，谷歌开始了一项实验，改变谷歌搜索中广告词广告的背景阴影颜色，并注意到实验组用户的广告点击量明显高于对照组。

#### 评价人评估

给定操作的结果呈现给人类评价者，并选择哪个“更好”以及为什么。然后，该反馈用于确定给定的变化是积极的、中性的还是消极的。例如，谷歌历史上一直使用评分者评估进行搜索查询(我们发布了我们给评分者的指南)。在某些情况下，来自该评级数据的反馈可以帮助确定算法更改的启动/不启动。评价者的评价对于机器学习系统这样的非官方系统来说至关重要，因为在这类系统中，没有明确的正确答案，只有好坏的概念。

# 大型测试以及开发者工作流程

我们已经讨论了什么是大型测试，为什么要进行大型测试，什么时候进行大型测试，以及要进行多少大型测试，但是我们没有过多地讨论世卫组织。谁写测试？谁运行测试并调查失败？谁拥有这些测试？我们如何让这一切变得可以忍受？

尽管标准的单元测试基础设施可能不适用，但是将更大的测试集成到开发人员的工作流程中仍然是至关重要的。这样做的一个方法是确保存在提交前和提交后执行的自动化机制，即使这些机制不同于单元测试的机制。在谷歌，许多这样的大型测试不属于TAP。它们不封闭，太脆弱，和/或太耗费资源。但我们仍然需要防止它们被破坏，否则它们不会提供信号，变得太难分类。然后，我们要做的是为这些进行单独的提交后连续构建。我们也鼓励在提交之前运行这些测试，因为这样可以直接向作者提供反馈。

需要手动批准差异的A/B差异测试也可以合并到这样的工作流中。对于预提交，在批准变更之前，批准用户界面中的任何差异可能是一个代码审查要求。一个这样的测试是，如果提交的代码有未解决的差异，我们会自动阻止文件发布错误。

在某些情况下，测试太大或太痛苦，以至于预提交的执行增加了太多的开发人员的摩擦。这些测试仍然在提交后运行，并且也作为发布过程的一部分运行。不运行这些预提交的缺点是，污点会进入单一报告，我们需要确定罪魁祸首的变化来回滚它。但我们需要在开发人员的痛苦、变更延迟和持续构建的可靠性之间进行权衡。

## 创作大型测试

虽然大型测试的结构相当标准，但是创建这样的测试仍然存在挑战，尤其是如果这是团队中的第一次有人这样做。

使编写这样的测试成为可能的最好方法是拥有清晰的库、文档和示例。由于本地语言支持，单元测试很容易编写(JUnit曾经很深奥，但现在是主流)。我们在功能集成测试中重用这些断言库，但是我们也随着时间的推移创建了用于与SUTs交互、运行A/B差异、播种测试数据以及执行测试工作流的库。

大型测试的维护成本更高，包括资源和人力，但并非所有大型测试都是同等的。A/B差异测试流行的一个原因是它们在维护验证步骤中的人力成本更低。类似地，生产性水下生产系统比隔离密封水下生产系统的维护成本更低。因为所有这些编写的基础设施和代码都必须得到维护，所以成本节约会更加复杂。

但是，这个成本必须从整体上看。如果手动调节差异或支持和保护生产测试的成本超过了节约的成本，那么它就变得无效。

## 运行大型测试

我们上面提到了我们的大型测试不适合TAP，所以我们为它们提供了交替的连续构建和预提交。对我们的工程师来说，最初的挑战之一是如何运行非标准测试，以及如何迭代它们。

我们尽可能地尝试让我们的大型测试以我们的工程师熟悉的方式运行。我们的预提交基础结构在运行这些测试和运行TAP测试之前放置了一个公共的API，我们的代码审查基础结构显示了两组结果。但是许多大型测试是定制的，因此需要特定的文档来说明如何按需运行它们。对于不熟悉的工程师来说，这可能是一个挫折的来源。

### 加速测试

工程师不会等待缓慢的测试。测试越慢，工程师运行测试的频率就越低，测试失败后等待的时间就越长，直到测试再次通过。

加快测试速度的最好方法通常是缩小测试范围，或者将一个大型测试分成两个可以并行运行的较小测试。但是还有一些其他的技巧可以用来加速更大的测试。

一些简单的测试会使用基于时间的休眠来等待不确定的动作发生，这在大型测试中很常见。然而，这些测试没有线程限制，真正的生产用户希望尽可能少地等待，所以测试最好像真正的生产用户那样反应。方法包括如下:
• 在一个时间窗口内重复轮询状态转换，以使事件以接近微秒的频率结束。您可以将其与超时值相结合，以防测试未能达到稳定状态。
• 实现事件处理程序。
• 订阅事件完成通知系统。

请注意，当运行的这些测试超载时，依赖睡眠和超时的测试都将开始失败，这是因为这些测试需要更频繁地重新运行，从而进一步增加了负载。

#### 降低内部系统超时和延迟

生产系统通常采用分布式部署拓扑进行配置，但SUT系统可能部署在单台机器上(或至少部署在一组机器上)。如果生产代码中存在硬编码超时或(特别是)睡眠语句，以解决生产系统延迟问题，那么在运行测试时，应该对这些语句进行调整和减少。

#### 优化测试构建时间

我们的Monorepo的一个缺点是，大型测试的所有依赖都是作为输入构建和提供的，但是对于一些大型测试来说，这可能不是必需的。如果SUT由真正成为测试焦点的核心部分和一些其他必要的对等二进制依赖项组成，则有可能在已知的好版本上使用这些其他二进制文件的预构建版本。我们构建的系统(基于monorepo)不容易支持这种模型，但这种方法实际上更能反映不同服务在不同版本下发布的生产情况。

对于单元测试来说，剥落已经够糟糕的了，但是对于更大的测试来说，它会使它们无法使用。

### 去除碎片化

对于单元测试来说，碎片化已经够糟糕的了，但是对于更大的测试来说，它会使它们无法使用。一个团队应该把消除这种测试的碎片化作为一个高优先级。但是如何从这样的测试中去除碎片化呢？

最大限度地减少碎片化从减少测试范围开始——封闭式SUT不会有多用户和现实世界生产或共享暂存环境碎片化的风险，单机封闭式SUT不会有分布式SUT的网络和部署碎片化问题。但是您可以通过测试设计和实施以及其他技术来减轻其它碎片化问题。在某些情况下，您需要平衡这些与测试速度。

正如使测试反应性或事件驱动可以加速测试一样，它也可以消除碎片化。定时睡眠需要超时维护，这些超时可以嵌入到测试代码中。增加内部系统超时可以减少碎片化，而如果系统以不确定的方式运行，减少内部超时会导致碎片化。这里的关键是确定一个折衷方案，既能为最终用户定义可容忍的系统行为(例如，我们允许的最大超时是n秒)，又能很好地处理不稳定的测试执行行为。

内部系统超时的一个更大问题是，超过它们会导致难以分类的错误。生产系统通常会试图通过优雅地处理可能的内部系统问题，将最终用户暴露限制在灾难性故障范围内。例如，如果谷歌不能在给定的时间限制内提供广告，我们不会退回500英镑，我们只是不提供广告。但是在测试运行人员看来，这就好像广告服务代码可能会在出现不稳定的超时问题时被破坏。在这种情况下，重要的是要使故障模式显而易见，并使测试场景的这种内部超时调整变得容易。

### 让测试变得容易理解

很难将测试集成到开发人员工作流程中的一个具体情况是，这些测试产生的结果对于运行测试的工程师来说是难以理解的。即使是单元测试也会产生一些混乱——如果我的改变破坏了你的测试，如果我对你的代码不熟悉，就很难理解为什么——但是对于更大的测试来说，这种混乱是不可克服的。果断的测试必须提供明确的通过/失败信号，并且必须提供有意义的错误输出，以帮助对失败源进行分类。需要人工调查的测试，如甲乙差异测试，需要特殊处理才能有意义，否则在提交前有被跳过的风险。

这在实践中是如何运作的？失败的大型测试应该执行以下操作:

#### 有一个明确指出失败原因的信息

最坏的情况是出现一个只说“断言失败”的错误和一个堆栈跟踪。一个好的错误是预测测试运行人员对代码的不熟悉程度，并提供一条给出上下文的消息:“In test_ReturnsOneFullPageOfSearchResultsForAPopularQuery, expected 10 search results but got 1.”对于未通过的性能测试或甲乙差异测试，应在输出中明确解释所测量的内容以及为什么该行为被认为是可疑的。

#### 最小化确定差异的根本原因的难度

堆栈跟踪对大型测试没有用，因为调用链可以跨越多个进程边界。相反，有必要在整个调用链中生成一个跟踪，或者投资于自动化，以缩小问题的范围。这个测试应该会产生某种人工产物。例如，Dapper是Google使用的一个框架，用于将单个请求标识与RPC调用链中的所有请求相关联，并且该请求的所有相关日志都可以通过该标识进行关联，以方便跟踪。

#### 提供支持和联系信息

通过使测试的所有者和支持者易于联系，测试者应该很容易获得帮助。

## 拥有大型测试

大型测试必须有记录在案的所有者——能够充分审查测试变更的工程师，以及在测试失败的情况下能够提供支持的工程师。如果没有适当的所有权，测试可能会成为以下情况的受害者:
• 贡献者修改和更新测试变得更加困难
• 解决测试失败需要更长的时间

然后测试会变得腐坏。

特定项目中组件的集成测试应该归项目负责人所有。面向特性的测试(覆盖一组服务的特定业务特性的测试)应该由“特性所有者”拥有；在某些情况下，这个所有者可能是一个负责端到端特性实现的软件工程师；在其他情况下，拥有业务场景描述的可能是产品经理或“测试工程师”。无论谁拥有测试，都必须被授权来确保它的整体健康，并且必须既有能力支持它的维护，又有这样做的动机。

如果这些信息是以结构化的方式记录的，那么围绕测试所有者构建自动化是可能的。我们使用的一些方法包括:

#### 常规代码所有权

在许多情况下，一个更大的测试是一个独立的代码工件，它位于我们代码库中的一个特定位置。在这种情况下，我们可以使用Monorepo中已经存在的拥有者(第9章)信息来提示自动化，特定测试的所有者是测试代码的所有者。

#### 测试注释

在某些情况下，可以将多个测试方法添加到一个测试类或模块中，并且这些测试方法中的每一个都可以有不同的特性所有者。我们使用了每种语言的结构化注释去记录了每种情况下的测试所有者，在这种情况下，如果某个特定的测试方法失败了，我们就可以确定要联系的所有者。

# 结论

一个全面的测试套件需要更大的测试，既要确保测试与被测系统的保真度相匹配，又要解决单元测试不能完全覆盖的问题。因为这样的测试必然更加复杂并且运行速度更慢，所以必须小心确保这样的大型测试被正确地拥有、良好地维护，并且在必要时运行(例如在部署到生产之前)。总的来说，这样的大型测试必须尽可能的小(同时保持保真度)，以避免开发人员的摩擦。对大多数软件项目来说，识别系统风险的综合测试策略以及解决这些风险的大型测试是必要的。

# TL;DRs

• 更大的测试覆盖了单元测试不能覆盖的东西。
• 大型测试由测试中的系统、数据、动作和验证组成。
• 好的设计包括识别风险的测试策略和减轻风险的大型测试。
• 必须对更大的测试做出额外的努力，以防止它们在开发人员工作流程中产生摩擦。

